{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\lenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfFileReader\n",
    "import textract\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfFileReader\n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"File not found:\", file_path)\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        text = textract.process(file_path, method='pdfminer')\n",
    "        return text.decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while extracting text:\", str(e))\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove noise, white spaces, and punctuation using regular expressions\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra white spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Define a function to perform part of speech tagging\n",
    "def pos_tagging(tokens):\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "\n",
    "# Define a function to extract bag of words and 2-grams\n",
    "def extract_bag_of_words(tokens):\n",
    "    # Bag of Words\n",
    "    word_freq = FreqDist(tokens)\n",
    "    most_common_words = word_freq.most_common()\n",
    "\n",
    "    # 2-grams\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    n_gram_scores = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    most_common_2grams = sorted(n_gram_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return most_common_words, most_common_2grams\n",
    "\n",
    "\n",
    "# Define a function for text summarization\n",
    "def text_summarization(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Extract the most frequent words\n",
    "    tokens = word_tokenize(text)\n",
    "    word_freq = FreqDist(tokens)\n",
    "    most_common_words = [word for word, _ in word_freq.most_common(10)]\n",
    "\n",
    "    # Select short sentences that contain the most frequent words\n",
    "    selected_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = word_tokenize(sentence)\n",
    "        common_words_count = sum(1 for token in sentence_tokens if token in most_common_words)\n",
    "        if common_words_count >= 3:\n",
    "            selected_sentences.append(sentence)\n",
    "\n",
    "    return selected_sentences\n",
    "\n",
    "\n",
    "# Define a function for sentiment analysis\n",
    "def sentiment_analysis(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "\n",
    "# Define a function for topic modelling (Latent Dirichlet Allocation)\n",
    "def latent_dirichlet_allocation(tokens, num_topics=3):\n",
    "    if len(tokens) == 0:\n",
    "        print(\"No tokens available for topic modeling.\")\n",
    "        return []\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([' '.join(tokens)])\n",
    "\n",
    "    if len(vectorizer.get_feature_names()) == 0:\n",
    "        print(\"Empty vocabulary. The documents may only contain stop words.\")\n",
    "        return []\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "\n",
    "    topic_keywords = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_indices = topic.argsort()[:-11:-1]\n",
    "        topic_keywords.append([feature_names[i] for i in top_indices])\n",
    "\n",
    "    return topic_keywords\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to generate a word cloud\n",
    "def generate_word_cloud(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while extracting text: expected str, bytes or os.PathLike object, not NoneType\n",
      "No tokens available for topic modeling.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m sentiment_scores \u001b[39m=\u001b[39m sentiment_analysis(text)\n\u001b[0;32m     12\u001b[0m topic_keywords \u001b[39m=\u001b[39m latent_dirichlet_allocation(preprocessed_text, num_topics\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m generate_word_cloud(preprocessed_text)\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mProcessed file:\u001b[39m\u001b[39m\"\u001b[39m, file_name)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMost common words:\u001b[39m\u001b[39m\"\u001b[39m, most_common_words)\n",
      "Cell \u001b[1;32mIn[3], line 134\u001b[0m, in \u001b[0;36mgenerate_word_cloud\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_word_cloud\u001b[39m(tokens):\n\u001b[0;32m    133\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(tokens)\n\u001b[1;32m--> 134\u001b[0m     wordcloud \u001b[39m=\u001b[39m WordCloud(width\u001b[39m=\u001b[39;49m\u001b[39m800\u001b[39;49m, height\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m, background_color\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mwhite\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mgenerate(text)\n\u001b[0;32m    136\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m    137\u001b[0m     plt\u001b[39m.\u001b[39mimshow(wordcloud, interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\wordcloud\\wordcloud.py:639\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m    625\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[39m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39m    self\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_text(text)\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\wordcloud\\wordcloud.py:621\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[39mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mself\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    620\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 621\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_frequencies(words)\n\u001b[0;32m    622\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\wordcloud\\wordcloud.py:410\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    408\u001b[0m frequencies \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(frequencies\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39mitemgetter(\u001b[39m1\u001b[39m), reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(frequencies) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWe need at least 1 word to plot a word cloud, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(frequencies))\n\u001b[0;32m    412\u001b[0m frequencies \u001b[39m=\u001b[39m frequencies[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_words]\n\u001b[0;32m    414\u001b[0m \u001b[39m# largest entry will be 1\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "folder_path = \"Articles/\"\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        tagged_tokens = pos_tagging(preprocessed_text)\n",
    "        most_common_words, most_common_2grams = extract_bag_of_words(preprocessed_text)\n",
    "        selected_sentences = text_summarization(text)\n",
    "        sentiment_scores = sentiment_analysis(text)\n",
    "        topic_keywords = latent_dirichlet_allocation(preprocessed_text, num_topics=5)\n",
    "        generate_word_cloud(preprocessed_text)\n",
    "\n",
    "        print(\"Processed file:\", file_name)\n",
    "        print(\"Most common words:\", most_common_words)\n",
    "        print(\"Most common 2-grams:\", most_common_2grams)\n",
    "        print(\"Selected sentences:\", selected_sentences)\n",
    "        print(\"Sentiment scores:\", sentiment_scores)\n",
    "        print(\"Topic keywords:\", topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while extracting text: [Errno 13] Permission denied: 'Articles/'\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"Articles/\"\n",
    "extract_text_from_pdf(\"Articles/\")\n",
    "file_path = os.path.join(folder_path, \"Article1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while extracting text: [Errno 13] Permission denied: 'Articles/'\n"
     ]
    }
   ],
   "source": [
    "extract_text_from_pdf(\"Articles/\")\n",
    "file_path = os.path.join(\"\", \"local_file.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Articles/article2.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(url)\n",
    "with open('local_file.pdf', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "print('PDF file downloaded successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Articles/article1.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m exported_file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mArticles/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39m# Copy the downloaded file to the exported file path\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m shutil\u001b[39m.\u001b[39;49mcopyfile(downloaded_file_path, exported_file_path)\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\shutil.py:264\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    262\u001b[0m     os\u001b[39m.\u001b[39msymlink(os\u001b[39m.\u001b[39mreadlink(src), dst)\n\u001b[0;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(src, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fsrc:\n\u001b[0;32m    265\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(dst, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fdst:\n\u001b[0;32m    267\u001b[0m                 \u001b[39m# macOS\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Articles/article1.pdf'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the path of the downloaded PDF file\n",
    "downloaded_file_path = url\n",
    "\n",
    "# Specify the path and name for the exported PDF file\n",
    "exported_file_path = 'Articles/'\n",
    "\n",
    "# Copy the downloaded file to the exported file path\n",
    "shutil.copyfile(downloaded_file_path, exported_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDA_georgia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
