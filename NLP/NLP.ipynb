{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\lenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.probability import FreqDist\n",
    "import stanza\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "stanza.download('en',verbose=False) # Load the English POS tagging model from Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pdfs from our S3 bucket\n",
    "#import requests\n",
    "#\n",
    "#url = 'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Articles/article10.pdf'\n",
    "#\n",
    "#response = requests.get(url)\n",
    "#with open('local_file.pdf', 'wb') as f:\n",
    "#    f.write(response.content)\n",
    "#print('PDF file downloaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"File not found:\", file_path)\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            text = \"\"\n",
    "\n",
    "            for page_number in range(num_pages):\n",
    "                page = pdf_reader.pages[page_number]\n",
    "                text += page.extract_text()\n",
    "\n",
    "        # Remove linebreaks\n",
    "        text = text.replace('\\n', ' ')\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while extracting text:\", str(e))\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text, extra_stop_words=[]):\n",
    "    # Remove noise, white spaces, and punctuation using regular expressions\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra white spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d', '', text)  # Remove numbers\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add extra stopwords\n",
    "    stop_words.update(extra_stop_words)  \n",
    "    # stop_words.update([\"noise\"]) # not sure whether or not to put noise as a stopword\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove tokens containing numbers or the word \"http\", remove stopwords, remove tokens of length <=2\n",
    "    tokens = [token for token in tokens if not any(char.isdigit() for char in token)\n",
    "              and \"http\" not in token and \"www\" not in token and token.lower() not in stop_words\n",
    "              and not (len(token) <= 2)]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stop_words.update([\"study\"]) # this can only be added here to the stop words or it will still appear in the results (because can be noun, verb, ...)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Define a function to perform part of speech tagging     # THIS FUNCTION GIVES BAD RESULTS, a lot of words are incorrectly labeled\n",
    "#def pos_tagging(tokens):\n",
    "#    nlp = stanza.Pipeline('en', processors='tokenize, mwt, pos') # POSProcessor requires the TokenizeProcessor and MWTProcessor in the pipeline\n",
    "#\n",
    "#   # Perform POS tagging with Stanza on each token\n",
    "#   doc = nlp(' '.join(tokens))\n",
    "#   tagged_tokens = [(word.text, word.upos) for sent in doc.sentences for word in sent.words]\n",
    "#\n",
    "#    # Split the tagged tokens into the ones without and with \"X\" as POS tag (= words that don't fit into a category)\n",
    "#    filtered_tagged_tokens = [(token, pos) for token, pos in tagged_tokens if pos != 'X']\n",
    "#    x_tagged_tokens = [(token, pos) for token, pos in tagged_tokens if pos == 'X']\n",
    "#\n",
    "#    # Print the tokens with POS tag 'X'\n",
    "#    print(\"\\nTokens with POS tag 'X' that are now removed:\")\n",
    "#    for token, pos in x_tagged_tokens:\n",
    "#        print(token, pos)\n",
    "#    \n",
    "#    return filtered_tagged_tokens\n",
    "\n",
    "\n",
    "def ner_extraction(tokens):\n",
    "    text = ' '.join(tokens)  # Convert preprocessed tokens back to text\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize, ner', verbose=False)\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.type) for sent in doc.sentences for ent in sent.ents]\n",
    "    return entities\n",
    "\n",
    "\n",
    "# Define a function to extract bag of words and 2-grams\n",
    "def extract_bag_of_words(tokens):\n",
    "    # Bag of Words\n",
    "    word_freq = FreqDist(tokens)\n",
    "    most_common_words = word_freq.most_common()\n",
    "\n",
    "    # 2-grams\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    n_gram_scores = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    most_common_2grams = sorted(n_gram_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return most_common_words, most_common_2grams\n",
    "\n",
    "# Define a function to summarize the text with sentences that contain many common words\n",
    "def text_summarization(text, common_words_list, extra_stopwords=[]): #possibility to add words that may not appear in the selected sentences; default of extra stopwords is empty\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Extract the 10 most frequent words from the bag_of_words results ((extra) stopwords don't occur in these results)\n",
    "    common_words = [word for word, _ in common_words_list[:10]]  # Extract only the words and not the frequencies\n",
    "\n",
    "    # Select short sentences that contain the most frequent words\n",
    "    selected_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = [token.lower() for token in word_tokenize(sentence)]  # Convert tokens to lowercase\n",
    "        common_words_count = sum(1 for token in sentence_tokens if token in common_words)\n",
    "        if common_words_count >= 6 and not any(word in sentence_tokens for word in extra_stopwords): #extra_stopwords may NOT appear in the sentence, default stopwords are allowed\n",
    "            selected_sentences.append(sentence)\n",
    "\n",
    "    # Check if any selected sentences were found\n",
    "    if len(selected_sentences) == 0:\n",
    "        print(\"No sentences found where common words occur more than once.\")\n",
    "\n",
    "    return selected_sentences\n",
    "\n",
    "\n",
    "# Define a function for sentiment analysis\n",
    "def sentiment_analysis(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "\n",
    "# Define a function to generate a word cloud\n",
    "def generate_word_cloud(tokens, max_words=20):\n",
    "    text = ' '.join(tokens)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=max_words).generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define a function for topic modelling (Latent Dirichlet Allocation)\n",
    "def latent_dirichlet_allocation(tokens, num_topics=3):\n",
    "    if len(tokens) == 0:\n",
    "        print(\"No tokens available for topic modeling.\")\n",
    "        return []\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([' '.join(tokens)])\n",
    "\n",
    "    if len(vectorizer.get_feature_names()) == 0:\n",
    "        print(\"Empty vocabulary. The documents may only contain stop words.\")\n",
    "        return []\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "\n",
    "    topic_keywords = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_indices = topic.argsort()[:-11:-1]\n",
    "        topic_keywords.append([feature_names[i] for i in top_indices])\n",
    "\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9191948288 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m preprocessed_text \u001b[39m=\u001b[39m preprocess_text(concatenated_text, extra_stop_words)\n\u001b[0;32m     19\u001b[0m \u001b[39m#tagged_tokens = pos_tagging(preprocessed_text)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m named_entities \u001b[39m=\u001b[39m ner_extraction(preprocessed_text) \n\u001b[0;32m     21\u001b[0m most_common_words, most_common_2grams \u001b[39m=\u001b[39m extract_bag_of_words(preprocessed_text)\n\u001b[0;32m     22\u001b[0m selected_sentences \u001b[39m=\u001b[39m text_summarization(concatenated_text, most_common_words, extra_stop_words)\n",
      "Cell \u001b[1;32mIn[38], line 83\u001b[0m, in \u001b[0;36mner_extraction\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     81\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(tokens)  \u001b[39m# Convert preprocessed tokens back to text\u001b[39;00m\n\u001b[0;32m     82\u001b[0m nlp \u001b[39m=\u001b[39m stanza\u001b[39m.\u001b[39mPipeline(lang\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m, processors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtokenize, ner\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 83\u001b[0m doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m     84\u001b[0m entities \u001b[39m=\u001b[39m [(ent\u001b[39m.\u001b[39mtext, ent\u001b[39m.\u001b[39mtype) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msentences \u001b[39mfor\u001b[39;00m ent \u001b[39min\u001b[39;00m sent\u001b[39m.\u001b[39ments]\n\u001b[0;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m entities\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\stanza\\pipeline\\core.py:464\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc, processors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(doc, processors)\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\stanza\\pipeline\\core.py:415\u001b[0m, in \u001b[0;36mPipeline.process\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors\u001b[39m.\u001b[39mget(processor_name):\n\u001b[0;32m    414\u001b[0m         process \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mbulk_process \u001b[39mif\u001b[39;00m bulk \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mprocess\n\u001b[1;32m--> 415\u001b[0m         doc \u001b[39m=\u001b[39m process(doc)\n\u001b[0;32m    416\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\stanza\\pipeline\\ner_processor.py:85\u001b[0m, in \u001b[0;36mNERProcessor.process\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m     83\u001b[0m     preds \u001b[39m=\u001b[39m []\n\u001b[0;32m     84\u001b[0m     \u001b[39mfor\u001b[39;00m i, b \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch):\n\u001b[1;32m---> 85\u001b[0m         preds \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(b)\n\u001b[0;32m     86\u001b[0m     all_preds\u001b[39m.\u001b[39mappend(preds)\n\u001b[0;32m     87\u001b[0m \u001b[39m# for each sentence, gather a list of predictions\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39m# merge those predictions into a single list\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m# earlier models will have precedence\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\stanza\\models\\ner\\trainer.py:113\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, batch, unsort)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m    112\u001b[0m \u001b[39m#batch_size = word.size(0)\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m _, logits, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(word, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)\n\u001b[0;32m    115\u001b[0m \u001b[39m# decode\u001b[39;00m\n\u001b[0;32m    116\u001b[0m trans \u001b[39m=\u001b[39m trans\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\stanza\\models\\ner\\model.py:175\u001b[0m, in \u001b[0;36mNERTagger.forward\u001b[1;34m(self, sentences, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_emb_dim\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcharlm\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 175\u001b[0m         char_reps_forward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcharmodel_forward\u001b[39m.\u001b[39;49mget_representation(chars[\u001b[39m0\u001b[39;49m], charoffsets[\u001b[39m0\u001b[39;49m], charlens, char_orig_idx)\n\u001b[0;32m    176\u001b[0m         char_reps_forward \u001b[39m=\u001b[39m PackedSequence(char_reps_forward\u001b[39m.\u001b[39mdata, char_reps_forward\u001b[39m.\u001b[39mbatch_sizes)\n\u001b[0;32m    177\u001b[0m         char_reps_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharmodel_backward\u001b[39m.\u001b[39mget_representation(chars[\u001b[39m1\u001b[39m], charoffsets[\u001b[39m1\u001b[39m], charlens, char_orig_idx)\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\stanza\\models\\common\\char_model.py:157\u001b[0m, in \u001b[0;36mCharacterLanguageModel.get_representation\u001b[1;34m(self, chars, charoffsets, charlens, char_orig_idx)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_representation\u001b[39m(\u001b[39mself\u001b[39m, chars, charoffsets, charlens, char_orig_idx):\n\u001b[0;32m    156\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 157\u001b[0m         output, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(chars, charlens)\n\u001b[0;32m    158\u001b[0m         res \u001b[39m=\u001b[39m [output[i, offsets] \u001b[39mfor\u001b[39;00m i, offsets \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(charoffsets)]\n\u001b[0;32m    159\u001b[0m         res \u001b[39m=\u001b[39m unsort(res, char_orig_idx)\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\stanza\\models\\common\\char_model.py:150\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[1;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m hidden \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \n\u001b[0;32m    148\u001b[0m     hidden \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharlstm_h_init\u001b[39m.\u001b[39mexpand(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_num_layers\u001b[39m\u001b[39m'\u001b[39m], batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_hidden_dim\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcontiguous(),\n\u001b[0;32m    149\u001b[0m               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharlstm_c_init\u001b[39m.\u001b[39mexpand(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_num_layers\u001b[39m\u001b[39m'\u001b[39m], batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_hidden_dim\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcontiguous())\n\u001b[1;32m--> 150\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcharlstm(embs, charlens, hx\u001b[39m=\u001b[39;49mhidden)\n\u001b[0;32m    151\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m])\n\u001b[0;32m    152\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(output)\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\stanza\\models\\common\\packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[1;34m(self, input, lengths, hx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, PackedSequence):\n\u001b[0;32m     20\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m pack_padded_sequence(\u001b[39minput\u001b[39m, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m---> 22\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx)\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad:\n\u001b[0;32m     24\u001b[0m     res \u001b[39m=\u001b[39m (pad_packed_sequence(res[\u001b[39m0\u001b[39m], batch_first\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)[\u001b[39m0\u001b[39m], res[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lenne\\anaconda3\\envs\\MDA_georgia\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:815\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[0;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[0;32m    817\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[0;32m    818\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9191948288 bytes."
     ]
    }
   ],
   "source": [
    "# Get the folder_path where the file is in (= current working directory)\n",
    "folder_path = os.getcwd()\n",
    "\n",
    "\n",
    "# Concatenate all article texts together\n",
    "concatenated_text = \"\" # initialize empty variable to store the concatenated_text in\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        concatenated_text += text\n",
    "\n",
    "# Define extra stop words: \"et\", \"al\", \"accessed\" and months to stopwords (to remove \"accessed {monthname}\"), ...\n",
    "extra_stop_words = [\"et\", \"al\", \"doi\", \"vol\", \"accessed\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"august\", \"september\", \"october\", \"november\", \"december\", \"table\", \"keywords\", \"model\", \"research\", \n",
    "                    \"downloaded\", \"guest\", \"bcn\", \"patient\", \"cluster\", \"model\", \"used\", \"results\", \"data\", \"analysis\", \"table\", \"value\", \"contents\", \"randomizations\", \"methods\", \"measurement\"]\n",
    "\n",
    "# Apply NLP to the concatenated text\n",
    "preprocessed_text = preprocess_text(concatenated_text, extra_stop_words)\n",
    "#tagged_tokens = pos_tagging(preprocessed_text)\n",
    "named_entities = ner_extraction(preprocessed_text) \n",
    "most_common_words, most_common_2grams = extract_bag_of_words(preprocessed_text)\n",
    "selected_sentences = text_summarization(concatenated_text, most_common_words, extra_stop_words)\n",
    "sentiment_scores = sentiment_analysis(concatenated_text)\n",
    "#topic_keywords_3 = latent_dirichlet_allocation(preprocessed_text)\n",
    "#topic_keywords_1 = latent_dirichlet_allocation(preprocessed_text, num_topics=1)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Processed all files\")\n",
    "Print(\"Named entities:\", named_entities)\n",
    "print(\"Most common words:\", most_common_words[:15])\n",
    "print(\"Most common 2-grams:\", most_common_2grams[:30])\n",
    "\n",
    "generate_word_cloud(preprocessed_text)\n",
    "print(f\"{len(selected_sentences)} selected sentences:\", selected_sentences)\n",
    "print(\"Sentiment scores:\", sentiment_scores)\n",
    "#print(\"Topic keywords:\", topic_keywords_3)\n",
    "#print(\"Topic keywords:\", topic_keywords_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jundishapur J Health Sci. 2015 January; 7(1): e25357. DOI: 10.5812/jjhs.25357   Published online 2015 January 20.  Research Article     Noise Pollution and Health Eﬀects   Sahar Geravandi  1 ; Afshin Takdastan  2 ; Elahe Zallaghi  3 ; Mehdi Vousoghi Niri  4 ; Mohammad  Javad Mohammadi  4,* ; Hamed Saki  4 ; Abolfazl Naiemabadi  4     1 Department of Nursing, Tehran Medical Sciences Branch, Islamic Azad University , Tehran, IR Iran  2 Environmental Technologies Research Center, Department of Environmental Health Engineering, Health Faculty , Ahvaz Jundishapur University of Medical Sciences, Ahvaz, IR Iran  3 Department of Environmental, Khouzestan Sciences and Research, Islamic Azad University , Ahvaz, IR Iran  4 Department of Environmental Health Engineering, Health Faculty , Ahvaz Jundishapur University of Medical Sciences, Ahvaz, IR Ir an    *Corresponding author  : Mohammad Javad Mohammadi, Department of Environmental Health Engineering, Health Faculty , Ahvaz Jundishapur University of Me dical Sciences, Ah- vaz, IR Iran. Tel: +98-9355439707, E-mail: javad_sam2000@yahoo.com, Mohamadi.m@ajums.ac.ir     Received:  November 15, 2014 ; Revised:  December 21, 2014 ; Accepted:  January 6, 2015    Background:   Noise pollution is of particular importance due to the physical and psychological eﬀects on humans. Noise is a stressor  that aﬀects the autonomic nervous system and the endocrine system. Noise is also a threat to marine and terrestrial ecosystems.  Health  risks from noise are correlated with road traﬃc. In other words, noise health eﬀects are the health consequences of elevated so und levels.   Objectives:   This study aims to determine the eﬀect of noise pollution (near roadways) on health issues in Ahvaz, Iran.   Materials and Methods:   In this cross-sectional study , equivalent sound pressure level were measured by sound level meters TES-1353 in  75 locations around 4 roadways, which had a high load of traﬃc in Ahvaz City during day time. During the study , 820 measurement s were  recorded at measuring stations, for 7 days per week with 1-hour interval between each measurement. Statistical analysis was car ried out  by SPSS software.  Results:   According to the research ﬁndings, the equivalent sound pressure levels in all stations were 76.28 ± 3.12 dB (Mean ± SD). Acc ording  to sound measurements and the survey questionnaire, noise pollution is higher than EPA (US Environmental Protection Agency) and  Iran  standard level. Based on result of this study the worst noise health eﬀects were the nervousness and sleep quality during 2012.    Conclusions:   According to the results of this study , with increasing load of traﬃc, there is an increasing need for proper consideration  plans to control noise pollution and prevent its eﬀects.     Keywords:  Noise; Health Impact Assessment; Sound  Copyright © 2015, Ahvaz Jundishapur University of Medical Sciences. This is an open-access article distributed under the terms of the Creative Commons Attribu- tion-NonCommercial 4.0 International License (http://creativecommons.org/licenses/by-nc/4.0/) which permits copy and redistribu te the material just in noncom- mercial usages, provided the original work is properly cited. 1. Background  Living in megacities has brought many problems for  its residents. In recent years, fast growing industry and vehicle population in the towns have resulted in consid-erable increase in traﬃc causing alarming noise and air pollution. Among the problems that can endanger hu-man peace and health is noise pollution (1-4). Noise is also a threat to marine and terrestrial ecosystems. Noise health eﬀects are the consequences of elevated sound lev-els (5). Exposure to road traﬃc noise is an important envi-ronmental problem that may cause hazardous eﬀects in communities (6-8).  Epidemiological studies have reported the association  between many medical problems such as myocardial infarction; cardiovascular disease; hypertension; sleep quality; mental disorders; immune system and birth de-fects; and exposure to road traﬃc noise; however, the cor-relations between noise frequency characteristics are not clear (9-16). Hypertension, increase secretion of adrena-line hormone, and heart rate are the most important complications of loud noises (17, 18). Noise level increases with traﬃc volume in an exponential rate. Aircrafts, mo-tor vehicles, subway and other transportation systems  are signiﬁcant sources of city noise (17). Study results showed that noise pollution aﬀects inhab- itants of the megacities worldwide. The social costs of traf-ﬁc noise in EU22 (European Union of 22 member states) are more than €40 billion per year; passenger cars and lor-ries are responsible for the bulk of costs (19). According to the report of European Environment Agency , about 100 million persons are exposed to road traﬃc noise above 55 dB in European Union countries (20). Furthermore, it is believed that more than 200 million people worldwide are subject to the eﬀects of noise (17). A study in California demonstrated that noise was increasing at the rate of 6.7 dBA due to street traﬃc (21). Rahmani et al. studied the as-sociation between noise levels and developed two noise determinants in 2011 (22). In a similar work, Lee et al. as-sessed road traﬃc noise levels and estimated the human exposure in Seoul, Korea (23). Another study to evaluate noise level in Curitiba, Brazil showed that people who live or work nearby these roads, experience above standard noise levels (24). In megacities like Ahvaz, complications Geravandi S et al. Jundishapur J Health Sci. 2015;7(1):e25357 2from loud noises are one of the major factors hindering  the comfort of the residents. Ahvaz, the capital of Khuzestan Province with a popula- tion of approximately 1 million, and an area of 8152 km 2   is located between 48º and 49°29′ east of Greenwich me-ridian and between 31º45′ and the north of the equator (25-27). Since a long time ago, Ahvaz has been well known because of its industries as well as environmental pol-lution. In the last decade, increased motor vehicles and transportation on roadways have contributed to other environmental problems (25-28). Physical and psycho-logical eﬀects of loud noises have been well documented (25-28). Furthermore, study about health eﬀects of noise pollution in most megacities, particularly Ahvaz is im-portant. Therefore, we decided to assess health eﬀects of noise pollution, which has not been studied yet (1, 21).  2. Objectives  The main purpose of this research was to assess the  noise pollution and health eﬀects near roadways in Ah-vaz City in 2012.  3. Materials and Methods  This cross-sectional study was conducted in Ahvaz to  assess noise index on 4 roadways (Ayatollah–Behbahani, Pasdaran, Azadegan, and Golestan) in 75 test locations in year 2012. In seeking to measure noise levels, the highest traﬃc load on the main streets was targeted that hap-pens between 07:00 and 22:00. Next, 31 test locations were selected around the streets with the highest levels of traﬃc (Figure 1). Noise levels were measured using by a sound level meters TES-1353 with an analyzer, which was accurate and made based on international standards. This device was calibrated before and after use based  on stretcher international standards (29, 30). Sound level meters were placed at a height of 150cm above the ground. Equivalent noise sampling was taken at 1-h inter-vals. The tests were performed from08:00 to 20:00, in all 75 test locations. Measurements in study locations were performed near the roadways (29, 31). Samplings were done during week (1, 6, 31). In the next  step, we prepared questionnaires, which included items regarding the demographic characteristics and attitude of residents living near test locations about noise pollu-tion eﬀects, including myocardial infarction, depression, hypertension, sleep quality , mental health, decrease in hearing and nervousness. We used Cranach α (0.88) for de-termining validity and reliability of the questionnaire. The collected data were calculated and processed by Excel and SPSS (version 16). To analyze the data, we applied descrip-tive statistics, including frequency , percentage, and mean (± SD). The primary standard of sound pressure according to EPA (US Environmental Protection Agency) is 65 dB (32). The standard of sound pressure level according to the stan-dards of Iran Ministry of Health is 65 dB too (32) (Figure 2).  Figure 1.   Study Locations in Near Roadway  Figure 2.   Location of the Study Area and Sampling Station in Ahvaz, Khuzestan Province, South-West of IranGeravandi S et al. 3 Jundishapur J Health Sci. 2015;7(1):e25357 Table 1.    L max , L min  and L eq Measured Near Roadwaysin Ahvaz  City , During 2012  a  Estimate Parameter L eq (dB) L min (dB) L max (dB)  Minimum 61.21 57.25 69.12  Maximum 91.35 80.62 95.46  Mean 76.28 68.93 84.32  a  L max : Maximum sound Pressure Level, L min : Minimum sound  Pressure Level, L eq : Sound pressure Level.  Table 2.   Main Health Eﬀects Attributed to Noise Pollution Near  Roadways in Ahvaz City , During 2012 Health Eﬀects Attitude Participants, No. (%)  Myocardial infarction 11 (3.66)  Depression 14 (4.66)  Sleep quality 51 (17)  Decrease in hearing 46 (15.33)  Mental health 31 (10.33)  Hypertension 23 (7.66)  Nervousness 124 (41.33) 454035302520151050 health eﬀects attributed to noise pollutionAttitude participants (percents) myocardial infarctiondepressionsleep quality decrease hearinghypertensionnervousnessmental health3.66 4.661715.3310.337.6641.33  Figure 3.   Health Eﬀects Caused by Noise Reported by Subjects in Ahvaz  City During 2012 78 7674 72 70 68 6664 62 60 58 Stuied roadsSound Pressure (dB)Equivalent SoundPressure(dB) EPA StandardSoundPressure(dB) roadways Azadegan roadways Pasdaran roadways Golestan roadways Ayatollah-Behbahani72.1275.6274.28 73.65 65 65 65 65  Figure 4.   Relationship Between Standard Noise Pollution and Average  Noise Pollution Measured in Roadway in Ahvaz City During 2012 4. Results  Based on the results of this study , almost all partici- pants believed that noise pollution aﬀects humans. Results showed that approximately 70% of the partici-pants were male, and the mean age was 35.6 ± 10.21 y . The results showed that approximately 85% of the measure-ments were higher than international standards. Also in this study, average sound pressure Level in all stations was 76.28± 3.08 dB, which was higher than EPA and Iran standard (65 dB) (Table 1) (32). The Pasdaran and Azade-gan highways had the highest and the lowest equivalent sound pressure levels, respectively during 2012. Noise pollution parameters of the present study are shown in Tables 1 and 2, Figures 3 and 4. According to the ﬁndings in Table 2, nervousness, sleep quality , hypertension, de-crease in hearing, mental health, depression, and myo-cardial infarction were the main health eﬀects caused by the noise.  Figure 3 shows the health eﬀects caused by noise re- ported by subjects in terms of the frequency of complica-tions. Figure 4 shows that average measured noise pollu-tion in roadways in Ahvaz City (during 2012) was higher than EPA and also much higher than Iran standards In view of noise pollution measured, shows that average roadway Pasdaran was the highest value in compare an-other roadways during this year, respectively.  5. Discussion  According to the ﬁndings (including sound measure- ments and the survey questionnaire), noise pollution near roadways in Ahvaz is higher than EPA and Iran stan-dard level. Minimum and Maximum sound pressure lev-els were 84.32 ± 4.07 dB and 68.93 ± 4.14 dB, respectively . Based on the results of this study , 41.33% of nervousness was attributed to traﬃc noise. Also, 17% of disturbance in sleep quality was attributed to traﬃc noise too. High per-centage of the observed health endpoints was associated with high level of measured sound. Some studies reported similar equivalent levels for traf- ﬁc noise range. A meta-analysis of 14 studies on the as-sociation between road traﬃc noise and coronary heart diseases was carried out. Based on the result, Rahmani et al. showed noise level above standard in Iran (22). Accord-ing to the study of Mirzaei et al. in Zahedan, approximate-ly 62% of subjects reported that street noise was painful and nervousness is the main complication of the noise (17). Mirzaei showed that high and very high levels of noise have a negative eﬀect on the feelings of tranquility (66.5%) and sleep (66%). High percentages of the observed health endpoints caused by noise in this study were asso-ciated with high measure sound pressure levels of noise in Ahvaz. One study on the eﬀects of long-term exposure to road traﬃc noise on sleep quality , using question-naires and actimetry , showed that sleep quality is most aﬀected by noise levels (33). Results of this study are diﬀerent from with that of Geravandi S et al. Jundishapur J Health Sci. 2015;7(1):e25357 4Ohrsrom study with respect to the geographic, demo- graphic, and climate characteristics. In a research con-ducted in Kashan (Iran), the equivalent sound pressure level in high traﬃc points of the city was 81.7 dB 15 (34). Based on the results of our study , maximum sound pressure level was relatively higher because of greater measured sound pressure levels in Ahvaz. In a research conducted in Sanandaj (Iran), the equivalent level at some hours of the day was 85 dB (35). Study of Hume et al. (2012) support evidence that night-time noise is likely associated with cardiovascular disease and stroke in the elderly . In this study , the result of myocardial in-farction caused by noise is consistent with Hume study . In another study , Gan et al. showed that living within 50m of a highway in metropolitan Vancouver was as-sociated with 50% increase in the risk of coronary heart disease (12). Based on the results of our study , loud noise has eﬀects on myocardial infarction similar to Gan study . One cross-sectional study has found a higher but non-signiﬁcant prevalence of hypertension among male workers exposed to loud noises (36). The results of this study show that nervousness is associated with loud noise in Ahvaz. A study conducted in Messina, Italy showed L  eq  above 75 dB (37). The results this study shows  that measured sound pressure levels in Ahvaz are very high compared to Italy . Based on the results of this study and because of in- creasing traﬃc load and city population as well as noise pollution studies in other countries, there is an increas-ing need for proper involvement of traﬃc police and other relevant organizations to attend to this form of pollution. Although the results of this study are in line with results of other researches around the world, our geographic, demographic, and climate characteristics are diﬀerent and further studies are really needed. By increasing knowledge and awareness among residents regarding the eﬀects of noise pollution, a preventive so-lution may be achieved that decreases periods of noise pollution exposure. Because of the increasing trends of city population and traﬃc loads on roadway , there is a need for traﬃc management, using noise controllers, increased knowledge, planting barriers on both sides of the roads to absorb noise, using high speed reducers and community participation.  Acknowledgements  The authors would like to thank Azma Jonob Company  for the technical support and providing the facilities.  Authors’ Contributions  Study concept, design, and critical revision of the man- uscript for important intellectual content: Mohammad Javad Mohammadi, Sahar Geravandi, Elahe Zallaghi, Af-shin Takdastan, Hamed Saki, and Abolfazl Naiemabadi; Drafting of the manuscript and advisor: Mehdi Vosoughi; Conducting experiments: Elahe Zallaghi. Funding/Support  This study was ﬁnancially supported by Ahvaz Jundisha- pur University of Medical Sciences.   References  1.        Data Acquisition Methods for Estimate the Noise Generated by  the Road Traﬃc. In: Florea D, Cofaru C, Covaciu D, Timar J editors. .  3rd WSEAS Conference on Urban Planning and Transportation UPT.    2010 . 2.       Kephalopoulos S, Paviotti M, Anfosso-Ledee F, Van Maercke D,  Shilton S, Jones N. Advances in the development of common noise assessment methods in Europe: The CNOSSOS-EU frame-work for strategic environmental noise mapping.  Sci Total Envi- ron.   2014; 482-483  :400–10. 3.       Sogebi OA, Amoran OE, Iyaniwura CA, Oyewole EA. Awareness  and attitudes to noise and its hazards in motor parks in a sub-urban Nigerian town.  Niger Postgrad Med J.   2014; 21 (1):40–5. 4.       Harris DA.  Noise Control Manual for Residential Buildings.  New  York: MC Graw Hill; 1997. 5.       Maschke C. Stress Hormone Changes in Persons exposed to Sim- ulated Night Noise.  Noise Health.   2003; 5 (17):35–45. 6.       Naddaﬁ K, Yunesian M, Mesdaghinia AR, Mahvi AH, Asgari A.  Noise Pollution in Zanjan City in 2007.  ZUMS J.   2008; 16 (62):85–96. 7.       Sarnat JA, Golan R, Greenwald R, Raysoni AU, Kewada P, Winquist  A, et al. Exposure to traﬃc pollution, acute inﬂammation and autonomic response in a panel of car commuters.  Environ Res.    2014; 133 :66–76. 8.       Sorensen M, Luhdorf P, Ketzel M, Andersen ZJ, Tjonneland A,  Overvad K, et al. Combined eﬀects of road traﬃc noise and am-bient air pollution in relation to risk for stroke?  Environ Res.    2014; 133 :49–55. 9.       Banerjee D. Association between transportation noise and  cardiovascular disease: a meta-analysis of cross-sectional stud-ies among adult populations from 1980 to 2010.  Indian J Public  Health.   2014; 58 (2):84–91. 10.       Leon Bluhm G, Berglind N, Nordling E, Rosenlund M. Road traf- ﬁc noise and hypertension.  Occup Environ Med.   2007; 64 (2):122–6. 11.       Chang TY, Beelen R, Li SF, Chen TI, Lin YJ, Bao BY, et al. Road traﬃc  noise frequency and prevalent hypertension in Taichung, Tai-wan: a cross-sectional study .  Environ Health.   2014; 13 (1):37. 12.       Gan WQ , Tamburic L, Davies HW, Demers PA, Koehoorn M, Brauer  M. Changes in residential proximity to road traﬃc and the risk of death from coronary heart disease.  Epidemiology.   2010; 21 (5):642–9. 13.       Istamto T, Houthuijs D, Lebret E. Multi-country willingness  to pay study on road-traﬃc environmental health eﬀects: are people willing and able to provide a number?  Environ Health.    2014; 13 (1):35. 14.       Selander J, Bluhm G, Nilsson M, Hallqvist J, Theorell T, Willix P,  et al. Joint eﬀects of job strain and road-traﬃc and occupational noise on myocardial infarction.  Scand J Work Environ Health.    2013; 39 (2):195–203. 15.       Sorensen M, Andersen ZJ, Nordsborg RB, Jensen SS, Lillelund KG,  Beelen R, et al. Road traﬃc noise and incident myocardial infarc-tion: a prospective cohort study .  PLoS One.   2012; 7 (6). 16.       Sygna K, Aasvang GM, Aamodt G, Oftedal B, Krog NH. Road traﬃc  noise, sleep and mental health.  Environ Res.   2014; 131 :17–24. 17.       Mirzaei R, Ansari-Mogaddam A, Mohammadi M, Rakhshani F,  Salmanpor M. Noise Pollution in Zahedan and Residents’ Knowl-edge About Noise Pollution.  Health Scope.   2012; 1 (1):3–6. 18.       Powazka EE. A cross-sectional study of occupational noise  exposure and blood pressure in steelworkers.  Noise Health.    2003; 5 (17):15–22. 19.       den Boer LC, Schroten A.  Traﬃc noise reduction in Europe.   2008.  Available from: http://www.transportenvironment.org/sites/te/ﬁles/media/2008-02_traﬃc_noise_ce_delft_report.pdf. 20.       Roosli M. [Health eﬀects of environmental noise exposure].  Ther  Umsch.   2013; 70 (12):720–4. 21.       Gündogdu O, Gokdag M, Yuksel F. A traﬃc noise prediction meth- od based on vehicle composition using genetic algorithms.  Appl  Acoust.   2005; 66 (7):799–809.Geravandi S et al. 5 Jundishapur J Health Sci. 2015;7(1):e2535722.       Rahmani S, Mousavi SM, Kamali MJ. Modeling of road-traﬃc  noise with the use of genetic algorithm.  Appl Soft Comput.    2011; 11 (1):1008–13. 23.       Lee J, Gu J, Park H, Yun H, Kim S, Lee W, et al. Estimation of popula- tions exposed to road traﬃc noise in districts of Seoul metropoli-tan area of Korea.  Int J Environ Res Public Health.   2014; 11 (3):2729–40. 24.       Calixto A, Diniz FB, Zannin PHT. The statistical modeling of road  traﬃc noise in an urban setting.  Cities.   2003; 20 (1):23–9. 25.       Geravandi S, Mohammadi MJ, Goudarzi G, Ahmadi Angali K,  Neisi AK, Zalaghi E. Health eﬀects of exposure to particulate mat-ter less than 10 microns (PM10) in Ahvaz.  J Qazvin Univ Med Sci.    2014; 18 (5):45–53. 26.       Goudarzi G, Geravandi S, Vosoughi M, Mohammadi M, Neisi A,  Taghavirad SS. Cardiovascular deaths related to Carbon monox-ide Exposure in Ahvaz, Iran.  2014.   2014; 1 (3):6. 27.       Goudarzi G, Mohammadi MJ, Ahmadi Angali K, Neisi AK, Babaei  AA, Mohammadi B, et al. Estimation of Health Eﬀects Attributed to NO2 Exposure Using AirQ Model.  Arch Hyg Sci.   2011; 1 (2). 28.       Taghavirad SS, Mohammadi MJ. The a study on concentration of  betx vapors during winter in the department of ports and ship-ping located in one of the southern cities of iran.  Int J Current Life  Sci.  2014; 4 (9):5416–20. 29.       Fidell S. Nationwide urban noise survey .  J Acoust Soc Am.   1978;  64 (1):198–215. 30.       Golmohammadi R, Aliabadi M. Noise Pollution and its Irritat- ing Eﬀects in Hospitals of Hamadan, Iran.  J Health System Res.    2012; 7 (6):958–64. 31.       Wetzel E, Nicolas J, Andre P, Boreux JJ. Modelling the propaga- tion pathway of street-traﬃc noise: practical comparison of German guidelines and real-world measurements.  Appl Acoust.    1999; 57 (2):97–107. 32.       Kayvani N.  National Environmental Protection Organization.   1st  edTehran: Dayereh sabz; 2004. 33.       Ohrstrom E. Sleep Studies Before and After - Results and Compar- ison of Diﬀerent Methods.  Noise Health.   2002; 4 (15):65–7. 34.       Motallebi Kashani M, Hannani M, Akbari H, Almasi H. Noise Pol- lution Survey in Kashan City (2000-2001).  Feyz J.   2002; 6 (1):30–6. 35.       Reshadmanesh N, Shariat M, Imandel K. Evaluation of the  environmental health in Sanandaj.  J Kordestan Univ Med Sci .    1996; 1 (2):16–20. 36.       Chang TY, Liu CS, Young LH, Wang VS, Jian SE, Bao BY. Noise fre- quency components and the prevalence of hypertension in workers.  Sci Total Environ.   2012; 416  :89–96. 37.       Piccolo A, Plutino D, Cannistraro G. Evaluation and analysis of  the environmental noise of Messina, Italy .  Applied Acoustics.    2005; 66 (4):447–65. \n"
     ]
    }
   ],
   "source": [
    "# get an error above: debug by seeing if everything works for article 1\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Specify the PDF file name\n",
    "pdf_file_name = \"Article3.pdf\"\n",
    "\n",
    "# Construct the full path to the PDF file\n",
    "pdf_file_path = os.path.join(current_dir, pdf_file_name)\n",
    "\n",
    "text = extract_text_from_pdf(pdf_file_path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jundishapur',\n",
       " 'health',\n",
       " 'sci',\n",
       " 'jjhs',\n",
       " 'published',\n",
       " 'online',\n",
       " 'article',\n",
       " 'noise',\n",
       " 'pollution',\n",
       " 'health',\n",
       " 'eﬀects',\n",
       " 'sahar',\n",
       " 'geravandi',\n",
       " 'afshin',\n",
       " 'takdastan',\n",
       " 'elahe',\n",
       " 'zallaghi',\n",
       " 'mehdi',\n",
       " 'vousoghi',\n",
       " 'niri',\n",
       " 'mohammad',\n",
       " 'javad',\n",
       " 'mohammadi',\n",
       " 'hamed',\n",
       " 'saki',\n",
       " 'abolfazl',\n",
       " 'naiemabadi',\n",
       " 'department',\n",
       " 'nursing',\n",
       " 'tehran',\n",
       " 'medical',\n",
       " 'science',\n",
       " 'branch',\n",
       " 'islamic',\n",
       " 'azad',\n",
       " 'university',\n",
       " 'tehran',\n",
       " 'iran',\n",
       " 'environmental',\n",
       " 'technology',\n",
       " 'center',\n",
       " 'department',\n",
       " 'environmental',\n",
       " 'health',\n",
       " 'engineering',\n",
       " 'health',\n",
       " 'faculty',\n",
       " 'ahvaz',\n",
       " 'jundishapur',\n",
       " 'university']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_stop_words = [\"et\", \"al\", \"doi\", \"vol\", \"accessed\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"august\", \"september\", \"october\", \"november\", \"december\", \"table\", \"keywords\", \"model\", \"research\", \n",
    "                    \"downloaded\", \"guest\", \"bcn\", \"patient\", \"cluster\", \"model\", \"used\", \"results\", \"data\", \"analysis\", \"table\", \"value\", \"contents\", \"randomizations\", \"methods\", \"measurement\"]\n",
    "\n",
    "preprocessed_text = preprocess_text(text, extra_stop_words)\n",
    "preprocessed_text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('recent year', 'DATE'),\n",
       " ('billion per year', 'DATE'),\n",
       " ('million', 'CARDINAL'),\n",
       " ('two', 'CARDINAL'),\n",
       " ('one', 'CARDINAL'),\n",
       " ('approximately million', 'CARDINAL'),\n",
       " ('last decade', 'DATE')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = ner_extraction(preprocessed_text)\n",
    "entities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_extraction(text):\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,ner', verbose=False)\n",
    "\n",
    "    # Perform NER with Stanza on the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract named entities from the document\n",
    "    named_entities = [(ent.text, ent.type) for sent in doc.sentences for ent in sent.ents]\n",
    "    \n",
    "    return named_entities\n",
    "\n",
    "ner_extraction(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words, most_common_2grams = extract_bag_of_words(preprocessed_text)\n",
    "print(most_common_words[:10])\n",
    "print(most_common_2grams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the preprocessing of the words is added here too, tokenize the words back to sentences if it's possible (for this part)\n",
    "selected_sentences = text_summarization(text, most_common_words)\n",
    "selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = sentiment_analysis(text)\n",
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a problem\n",
    "topic_keywords = latent_dirichlet_allocation(preprocessed_text, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    word_freq = FreqDist(preprocessed_text)\n",
    "    most_common_words = [word for word, _ in word_freq.most_common(10)]\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD VERSION\n",
    "# Define a function for text summarization\n",
    "def text_summarization(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Extract the most frequent words\n",
    "    tokens = word_tokenize(text)\n",
    "    word_freq = FreqDist(tokens)\n",
    "    most_common_words = [word for word, _ in word_freq.most_common(10)]\n",
    "\n",
    "    # Select short sentences that contain the most frequent words\n",
    "    selected_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = word_tokenize(sentence)\n",
    "        common_words_count = sum(1 for token in sentence_tokens if token in most_common_words)\n",
    "        if common_words_count >= 3:\n",
    "            selected_sentences.append(sentence)\n",
    "\n",
    "    return selected_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDA_georgia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
