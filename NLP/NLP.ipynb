{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\lenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pdfs from our S3 bucket\n",
    "#import requests\n",
    "#\n",
    "#url = 'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Articles/article10.pdf'\n",
    "#\n",
    "#response = requests.get(url)\n",
    "#with open('local_file.pdf', 'wb') as f:\n",
    "#    f.write(response.content)\n",
    "#print('PDF file downloaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract text from PDF file\n",
    "def extract_text_from_pdf(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"File not found:\", file_path)\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            text = \"\"\n",
    "\n",
    "            for page_number in range(num_pages):\n",
    "                page = pdf_reader.pages[page_number]\n",
    "                text += page.extract_text()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while extracting text:\", str(e))\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove noise, white spaces, and punctuation using regular expressions\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra white spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Define a function to perform part of speech tagging\n",
    "def pos_tagging(tokens):\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "# Define a function to extract bag of words and 2-grams\n",
    "def extract_bag_of_words(tokens):\n",
    "    # Bag of Words\n",
    "    word_freq = FreqDist(tokens)\n",
    "    most_common_words = word_freq.most_common()\n",
    "\n",
    "    # 2-grams\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    n_gram_scores = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    most_common_2grams = sorted(n_gram_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return most_common_words, most_common_2grams\n",
    "\n",
    "\n",
    "# Define a function for text summarization\n",
    "def text_summarization(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Extract the most frequent words\n",
    "    tokens = word_tokenize(text)\n",
    "    word_freq = FreqDist(tokens)\n",
    "    most_common_words = [word for word, _ in word_freq.most_common(10)]\n",
    "\n",
    "    # Select short sentences that contain the most frequent words\n",
    "    selected_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = word_tokenize(sentence)\n",
    "        common_words_count = sum(1 for token in sentence_tokens if token in most_common_words)\n",
    "        if common_words_count >= 3:\n",
    "            selected_sentences.append(sentence)\n",
    "\n",
    "    return selected_sentences\n",
    "\n",
    "\n",
    "# Define a function for sentiment analysis\n",
    "def sentiment_analysis(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "\n",
    "# Define a function for topic modelling (Latent Dirichlet Allocation)\n",
    "def latent_dirichlet_allocation(tokens, num_topics=3):\n",
    "    if len(tokens) == 0:\n",
    "        print(\"No tokens available for topic modeling.\")\n",
    "        return []\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([' '.join(tokens)])\n",
    "\n",
    "    if len(vectorizer.get_feature_names()) == 0:\n",
    "        print(\"Empty vocabulary. The documents may only contain stop words.\")\n",
    "        return []\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "\n",
    "    topic_keywords = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_indices = topic.argsort()[:-11:-1]\n",
    "        topic_keywords.append([feature_names[i] for i in top_indices])\n",
    "\n",
    "    return topic_keywords\n",
    "\n",
    "\n",
    "# Define a function to generate a word cloud\n",
    "def generate_word_cloud(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the folder_path where the file is in (= current working directory)\n",
    "folder_path = os.getcwd()\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        tagged_tokens = pos_tagging(preprocessed_text)\n",
    "        most_common_words, most_common_2grams = extract_bag_of_words(preprocessed_text)\n",
    "        selected_sentences = text_summarization(text)\n",
    "        sentiment_scores = sentiment_analysis(text)\n",
    "        topic_keywords = latent_dirichlet_allocation(preprocessed_text, num_topics=5)\n",
    "        generate_word_cloud(preprocessed_text)\n",
    "\n",
    "        print(\"Processed file:\", file_name)\n",
    "        print(\"Most common words:\", most_common_words)\n",
    "        print(\"Most common 2-grams:\", most_common_2grams)\n",
    "        print(\"Selected sentences:\", selected_sentences)\n",
    "        print(\"Sentiment scores:\", sentiment_scores)\n",
    "        print(\"Topic keywords:\", topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDA_georgia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
