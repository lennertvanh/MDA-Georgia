{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pytz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and combining data\n",
    "### Meteo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building preprocessing pipeline\n",
    "# Step 1: Concatenate datasets\n",
    "def concatenate_datasets(dfs):\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Step 2: Convert UTC to CEST\n",
    "def convert_utc_to_cest(df):\n",
    "    cest_timezone = pytz.timezone('Europe/Brussels')\n",
    "    df['DATEUTC'] = df['DATEUTC'].apply(lambda x: pytz.utc.localize(x).astimezone(cest_timezone))\n",
    "    return df\n",
    "\n",
    "# Step 2: Drop columns\n",
    "def drop_columns(df):\n",
    "    columns_to_keep = ['LC_RAININ', 'LC_DAILYRAIN', 'LC_WINDDIR', 'LC_WINDSPEED', 'LC_TEMP_QCL3', 'Month', 'Day', 'Hour']  #there's less columns we keep than drop\n",
    "    columns_to_drop = set(df.columns) - set(columns_to_keep)\n",
    "    return df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 3: Check for percentage of missing values in each column\n",
    "def print_null_percentage(df):\n",
    "    null_percentage = df.isnull().sum() / len(df)\n",
    "    print('The percentage of missing values in each column')\n",
    "    print(null_percentage)\n",
    "    return df\n",
    "\n",
    "# Step 4: Forward fill missing values\n",
    "def forward_fill(df):\n",
    "    return df.ffill()\n",
    "\n",
    "# Step 5: Check whether there are missing values left\n",
    "def check_missing_values(df):\n",
    "    missing_values = df.isnull().sum()\n",
    "    print('Check whether there are missing values left')\n",
    "    print(missing_values)\n",
    "    return df\n",
    "\n",
    "# Step 6: Calculate summary statistics for daily rain sum\n",
    "def daily_rain_sum(df):\n",
    "    summary_stats = df['LC_DAILYRAIN'].describe()\n",
    "    print('Summary statistics for daily rain sum')\n",
    "    print(summary_stats)\n",
    "    return df\n",
    "\n",
    "# Step 7: Calculate fraction of non-zero values in the 'LC_DAILYRAIN' column\n",
    "def non_zero_fraction(df):\n",
    "    nonzero_count = np.count_nonzero(df['LC_DAILYRAIN'])\n",
    "    non_zero_frac = nonzero_count/len(df)\n",
    "    print(\"Fraction of non-zero values:\", non_zero_frac)\n",
    "    return df\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_meteo = Pipeline([\n",
    "    ('concatenate_datasets', FunctionTransformer(concatenate_datasets)),\n",
    "    ('convert_utc_to_cest', FunctionTransformer(convert_utc_to_cest)),\n",
    "    ('drop_columns', FunctionTransformer(drop_columns)),\n",
    "    ('print_null_percentage', FunctionTransformer(print_null_percentage)),\n",
    "    ('forward_fill', FunctionTransformer(forward_fill)),\n",
    "    ('check_missing_values', FunctionTransformer(check_missing_values)),\n",
    "    ('daily_rain_sum', FunctionTransformer(daily_rain_sum)),\n",
    "    ('non_zero_fraction', FunctionTransformer(non_zero_fraction))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading meteo data\n",
    "quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "years = ['2022']\n",
    "base_url_meteo = 'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Meteo+data/LC_{}{}.csv'\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    for quarter in quarters:\n",
    "        url = base_url_meteo.format(year, quarter)\n",
    "        df = pd.read_csv(url)\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of missing values in each column\n",
      "LC_RAININ       0.056770\n",
      "LC_DAILYRAIN    0.056770\n",
      "LC_WINDDIR      0.056770\n",
      "LC_WINDSPEED    0.056770\n",
      "Month           0.000000\n",
      "Day             0.000000\n",
      "Hour            0.000000\n",
      "LC_TEMP_QCL3    0.062285\n",
      "dtype: float64\n",
      "Check whether there are missing values left\n",
      "LC_RAININ       0\n",
      "LC_DAILYRAIN    0\n",
      "LC_WINDDIR      0\n",
      "LC_WINDSPEED    0\n",
      "Month           0\n",
      "Day             0\n",
      "Hour            0\n",
      "LC_TEMP_QCL3    0\n",
      "dtype: int64\n",
      "Summary statistics for daily rain sum\n",
      "count    5.546880e+06\n",
      "mean     1.319783e-03\n",
      "std      6.177559e-03\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.540000e-01\n",
      "Name: LC_DAILYRAIN, dtype: float64\n",
      "Fraction of non-zero values: 0.17391488548517364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LC_RAININ</th>\n",
       "      <th>LC_DAILYRAIN</th>\n",
       "      <th>LC_WINDDIR</th>\n",
       "      <th>LC_WINDSPEED</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>LC_TEMP_QCL3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-169.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.048027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.985849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-167.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.950322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.949550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-166.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.952268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LC_RAININ  LC_DAILYRAIN  LC_WINDDIR  LC_WINDSPEED  Month  Day  Hour  \\\n",
       "0        0.0           0.0      -169.0          0.43      1    1     0   \n",
       "1        0.0           0.0      -170.0          0.33      1    1     0   \n",
       "2        0.0           0.0      -167.0          0.46      1    1     0   \n",
       "3        0.0           0.0      -160.0          0.52      1    1     0   \n",
       "4        0.0           0.0      -166.0          0.51      1    1     0   \n",
       "\n",
       "   LC_TEMP_QCL3  \n",
       "0     13.048027  \n",
       "1     12.985849  \n",
       "2     12.950322  \n",
       "3     12.949550  \n",
       "4     12.952268  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the pipeline\n",
    "meteo_combined_df = pipeline_meteo.fit_transform(dfs)\n",
    "meteo_combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Concatenate all the dataframes into a single dataframe\n",
    "meteo_combined_df = pd.concat(dfs, ignore_index=True)\n",
    "meteo_combined_df.head()\n",
    "\n",
    "del dfs # deleting the individual meteo datasets\n",
    "\n",
    "# Drop the columns we won't use\n",
    "columns_to_keep = ['LC_RAININ', 'LC_DAILYRAIN', 'LC_WINDDIR', 'LC_WINDSPEED', 'LC_TEMP_QCL3', 'Month', 'Day', 'Hour']  #there's less columns we keep than drop\n",
    "columns_to_drop = set(meteo_combined_df.columns) - set(columns_to_keep)\n",
    "meteo_combined_df.drop(columns=columns_to_drop, inplace=True)\n",
    "meteo_combined_df.head()\n",
    "\n",
    "# check for percentage of missing values in each column\n",
    "print(meteo_combined_df.isnull().sum() / len(meteo_combined_df))\n",
    "\n",
    "# Forward fill missing values in the original DataFrame\n",
    "meteo_combined_df.ffill(inplace=True)\n",
    "\n",
    "# check whether there are missing values left\n",
    "print(meteo_combined_df.isnull().sum())\n",
    "\n",
    "# calculate summary statistics for daily rain sum\n",
    "summary_stats = meteo_combined_df['LC_DAILYRAIN'].describe()\n",
    "print(summary_stats)\n",
    "\n",
    "# Count the number of non-zero values in the 'LC_DAILYRAIN' column\n",
    "nonzero_count = np.count_nonzero(meteo_combined_df['LC_DAILYRAIN'])\n",
    "\n",
    "# Display the fraction of non-zero values\n",
    "print(\"Fraction of non-zero values:\", nonzero_count/len(meteo_combined_df))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>LC_RAININ</th>\n",
       "      <th>LC_DAILYRAIN</th>\n",
       "      <th>LC_WINDDIR</th>\n",
       "      <th>LC_WINDSPEED</th>\n",
       "      <th>LC_TEMP_QCL3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>-18.197324</td>\n",
       "      <td>0.389565</td>\n",
       "      <td>13.100358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-16.227891</td>\n",
       "      <td>0.222602</td>\n",
       "      <td>12.669197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-13.710884</td>\n",
       "      <td>0.217194</td>\n",
       "      <td>12.520271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-16.401361</td>\n",
       "      <td>0.178248</td>\n",
       "      <td>12.386194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-10.268707</td>\n",
       "      <td>0.237670</td>\n",
       "      <td>12.080706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month  Day  Hour  LC_RAININ  LC_DAILYRAIN  LC_WINDDIR  LC_WINDSPEED  \\\n",
       "0      1    1     0   0.000003       0.00036  -18.197324      0.389565   \n",
       "1      1    1     1   0.000007       0.00000  -16.227891      0.222602   \n",
       "2      1    1     2   0.000009       0.00000  -13.710884      0.217194   \n",
       "3      1    1     3   0.000000       0.00000  -16.401361      0.178248   \n",
       "4      1    1     4   0.000000       0.00000  -10.268707      0.237670   \n",
       "\n",
       "   LC_TEMP_QCL3  \n",
       "0     13.100358  \n",
       "1     12.669197  \n",
       "2     12.520271  \n",
       "3     12.386194  \n",
       "4     12.080706  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe per hour\n",
    "\n",
    "# Specify the aggregation function for each column\n",
    "  # for LC_DAILYRAIN we take the last value because it's cumulative, for other columns the mean\n",
    "aggregations = {\n",
    "    'LC_DAILYRAIN': 'mean',  # Select the last value for 'LC_DAILYRAIN' ###TAKE MEAN FOR NOW TO MAKE THE GRAPHS LOOK OK\n",
    "    'LC_RAININ': 'mean',  \n",
    "    'LC_WINDDIR': 'mean',\n",
    "    'LC_WINDDIR': 'mean', \n",
    "    'LC_WINDSPEED': 'mean', \n",
    "    'LC_TEMP_QCL3': 'mean'\n",
    "}\n",
    "\n",
    "# Perform the groupby aggregation\n",
    "meteo_per_hour = meteo_combined_df.groupby(['Month', 'Day', 'Hour']).mean()\n",
    "meteo_per_hour = meteo_per_hour.reset_index()\n",
    "meteo_per_hour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>LC_RAININ</th>\n",
       "      <th>LC_DAILYRAIN</th>\n",
       "      <th>LC_WINDDIR</th>\n",
       "      <th>LC_WINDSPEED</th>\n",
       "      <th>Hour</th>\n",
       "      <th>LC_TEMP_QCL3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-6.263419</td>\n",
       "      <td>0.324702</td>\n",
       "      <td>11.491857</td>\n",
       "      <td>12.194145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>-28.223994</td>\n",
       "      <td>0.707815</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>11.893159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>-34.624291</td>\n",
       "      <td>0.650430</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>9.539620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>-25.189413</td>\n",
       "      <td>0.326204</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>6.768886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>-43.943098</td>\n",
       "      <td>0.602983</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>3.889968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month  Day  LC_RAININ  LC_DAILYRAIN  LC_WINDDIR  LC_WINDSPEED       Hour  \\\n",
       "0      1    1   0.000002      0.000043   -6.263419      0.324702  11.491857   \n",
       "1      1    2   0.000671      0.003387  -28.223994      0.707815  11.500000   \n",
       "2      1    3   0.000729      0.006724  -34.624291      0.650430  11.500000   \n",
       "3      1    4   0.000454      0.003578  -25.189413      0.326204  11.500000   \n",
       "4      1    5   0.000127      0.000255  -43.943098      0.602983  11.500000   \n",
       "\n",
       "   LC_TEMP_QCL3  \n",
       "0     12.194145  \n",
       "1     11.893159  \n",
       "2      9.539620  \n",
       "3      6.768886  \n",
       "4      3.889968  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe per day\n",
    "\n",
    "# still the same \"aggregations\" as before\n",
    "meteo_per_day = meteo_combined_df.groupby(['Month', 'Day']).mean()\n",
    "meteo_per_day = meteo_per_day.reset_index()\n",
    "meteo_per_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>LC_RAININ</th>\n",
       "      <th>LC_DAILYRAIN</th>\n",
       "      <th>LC_WINDDIR</th>\n",
       "      <th>LC_WINDSPEED</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>LC_TEMP_QCL3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>-16.334089</td>\n",
       "      <td>0.336535</td>\n",
       "      <td>15.999657</td>\n",
       "      <td>11.499737</td>\n",
       "      <td>4.698144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>-25.171683</td>\n",
       "      <td>0.740747</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>6.923138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>12.190264</td>\n",
       "      <td>0.250732</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>8.113201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>3.643447</td>\n",
       "      <td>0.369271</td>\n",
       "      <td>15.500311</td>\n",
       "      <td>11.500246</td>\n",
       "      <td>10.704171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>-9.022240</td>\n",
       "      <td>0.240475</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>15.572697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month  LC_RAININ  LC_DAILYRAIN  LC_WINDDIR  LC_WINDSPEED        Day  \\\n",
       "0      1   0.000112      0.001033  -16.334089      0.336535  15.999657   \n",
       "1      2   0.000131      0.001257  -25.171683      0.740747  14.500000   \n",
       "2      3   0.000010      0.000105   12.190264      0.250732  16.000000   \n",
       "3      4   0.000054      0.000503    3.643447      0.369271  15.500311   \n",
       "4      5   0.000076      0.000653   -9.022240      0.240475  16.000000   \n",
       "\n",
       "        Hour  LC_TEMP_QCL3  \n",
       "0  11.499737      4.698144  \n",
       "1  11.500000      6.923138  \n",
       "2  11.500000      8.113201  \n",
       "3  11.500246     10.704171  \n",
       "4  11.500000     15.572697  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe per month\n",
    "\n",
    "# still the same \"aggregations\" as before\n",
    "meteo_per_month = meteo_combined_df.groupby(['Month']).mean()\n",
    "meteo_per_month = meteo_per_month.reset_index()\n",
    "meteo_per_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete combined dataframe\n",
    "del dfs\n",
    "del meteo_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframes (only needs to be ran once so comment it out)\n",
    "#meteo_per_hour.to_csv('hourly_weatherdata_2022.csv', index=False)\n",
    "#meteo_per_day.to_csv('daily_weatherdata_2022.csv', index=False)\n",
    "#meteo_per_month.to_csv('monthly_weatherdata_2022.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise level data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modelling data pipeline\n",
    "# Step 1: Concatenate datasets\n",
    "def concatenate_datasets(dfs):\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Step 2: Convert timestamps to datetime\n",
    "def convert_to_datetime(df):\n",
    "    df['result_timestamp'] = pd.to_datetime(df['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "    return df\n",
    "\n",
    "# Step 3: Extract month, day, hour, minute from timestamps\n",
    "def extract_month(df):\n",
    "    df['month'] = df['result_timestamp'].dt.month\n",
    "    return df\n",
    "def extract_day(df):\n",
    "    df['day'] = df['result_timestamp'].dt.day\n",
    "    return df\n",
    "def extract_hour(df):\n",
    "    df['hour'] = df['result_timestamp'].dt.hour\n",
    "    return df\n",
    "#def extract_minute(df):\n",
    "#   df['minute'] = df['result_timestamp'].dt.minute\n",
    "#   return df\n",
    "\n",
    "# Step 4: Drop columns\n",
    "def drop_columns(df):\n",
    "    columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "    columns_to_drop = set(df.columns) - set(columns_to_keep)\n",
    "    return df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_modelling = Pipeline([\n",
    "    ('concatenate_datasets', FunctionTransformer(concatenate_datasets)),\n",
    "    ('convert_to_datetime', FunctionTransformer(convert_to_datetime)),\n",
    "    ('extract_month', FunctionTransformer(extract_month)),\n",
    "    ('extract_day', FunctionTransformer(extract_day)),\n",
    "    ('extract_hour', FunctionTransformer(extract_hour)),\n",
    "    ('drop_columns', FunctionTransformer(drop_columns))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall dataset preprocessing pipeline\n",
    "# Step 1: Concatenate datasets\n",
    "def concatenate_datasets(dfs):\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Step 2: Convert timestamps to datetime\n",
    "def convert_to_datetime(df):\n",
    "    df['result_timestamp'] = pd.to_datetime(df['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "    return df\n",
    "\n",
    "# Step 3: Extract month, day, hour, minute from timestamps\n",
    "def extract_month(df):\n",
    "    df['month'] = df['result_timestamp'].dt.month\n",
    "    return df\n",
    "def extract_day(df):\n",
    "    df['day'] = df['result_timestamp'].dt.day\n",
    "    return df\n",
    "def extract_hour(df):\n",
    "    df['hour'] = df['result_timestamp'].dt.hour\n",
    "    return df\n",
    "#def extract_minute(df):\n",
    "#   df['minute'] = df['result_timestamp'].dt.minute\n",
    "#   return df\n",
    "\n",
    "# Step 4: Drop columns\n",
    "def drop_columns(df):\n",
    "    columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "    columns_to_drop = set(df.columns) - set(columns_to_keep)\n",
    "    return df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 5: Forward fill missing values\n",
    "def forward_fill(df):\n",
    "    return df.ffill()\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_general = Pipeline([\n",
    "    ('concatenate_datasets', FunctionTransformer(concatenate_datasets)),\n",
    "    ('convert_to_datetime', FunctionTransformer(convert_to_datetime)),\n",
    "    ('extract_month', FunctionTransformer(extract_month)),\n",
    "    ('extract_day', FunctionTransformer(extract_day)),\n",
    "    ('extract_hour', FunctionTransformer(extract_hour)),\n",
    "    ('drop_columns', FunctionTransformer(drop_columns)),\n",
    "    ('forward_fill', FunctionTransformer(forward_fill))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hourly pipeline\n",
    "# Step 6: Perform groupby to create dataframe per hour\n",
    "def perform_groupby(df):\n",
    "    return df.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "\n",
    "# Step 7: Reset index\n",
    "def reset_index_func(df):\n",
    "    return df.reset_index()\n",
    "\n",
    "# Step 8: Standardize the data\n",
    "def standardize_columns(df, columns_to_standardize):\n",
    "    scaler = StandardScaler()\n",
    "    standardized_values = scaler.fit_transform(df[columns_to_standardize])\n",
    "    new_columns = [column + '_standardized' for column in columns_to_standardize]\n",
    "    df[new_columns] = pd.DataFrame(standardized_values, columns=new_columns)\n",
    "    return df\n",
    "\n",
    "# Step 9: Define a custom transformer to create the new column\n",
    "class DateTransformer:\n",
    "    def transform(self, df):\n",
    "        df['year'] = 2022\n",
    "        df['date'] = df.apply(lambda row: pd.to_datetime(f\"{int(row['day']):02d}-{int(row['month']):02d}-{int(row['year']):04d}-{int(row['hour']):02d}\", format='%d-%m-%Y-%H'), axis=1)\n",
    "        df['date'] = df['date'].dt.strftime('%H:%M %d-%m-%Y')\n",
    "        return df\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "    \n",
    "# Step 10: Drop the year column\n",
    "def drop_year_column(df):\n",
    "    return df.drop(columns='year')\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_hourly = Pipeline([\n",
    "    ('groupby', FunctionTransformer(perform_groupby)),\n",
    "    ('reset_index', FunctionTransformer(reset_index_func)),\n",
    "    ('standardize_columns', FunctionTransformer(standardize_columns, kw_args={'columns_to_standardize': ['lamax', 'laeq']})),\n",
    "      ('date_transformer', DateTransformer()),\n",
    "    ('drop_year_column', FunctionTransformer(drop_year_column))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Daily pipeline\n",
    "# Step 6: Perform groupby to create dataframe per hour\n",
    "def perform_groupby(df):\n",
    "    return df.groupby(['month', 'day', 'description']).mean()\n",
    "\n",
    "# Step 7: Reset index\n",
    "def reset_index_func(df):\n",
    "    return df.reset_index()\n",
    "\n",
    "# Step 8: Standardize the data\n",
    "def standardize_columns(df, columns_to_standardize):\n",
    "    scaler = StandardScaler()\n",
    "    standardized_values = scaler.fit_transform(df[columns_to_standardize])\n",
    "    new_columns = [column + '_standardized' for column in columns_to_standardize]\n",
    "    df[new_columns] = pd.DataFrame(standardized_values, columns=new_columns)\n",
    "    return df\n",
    "\n",
    "# Step 9: Drop unwanted columns\n",
    "def drop_columns(df):\n",
    "    return df.drop(columns='hour')\n",
    "\n",
    "# Step 10: Define a custom transformer to create the new column\n",
    "class DateTransformer:\n",
    "    def transform(self, df):\n",
    "        df['year'] = 2022\n",
    "        df['date'] = df.apply(lambda row: pd.to_datetime(f\"{int(row['day']):02d}-{int(row['month']):02d}-{int(row['year']):04d}\", format='%d-%m-%Y'), axis=1)\n",
    "        df['date'] = df['date'].dt.strftime('%d-%m-%Y')\n",
    "        return df\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "    \n",
    "# Step 11: Drop the year column\n",
    "def drop_year_column(df):\n",
    "    return df.drop(columns='year')\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_daily = Pipeline([\n",
    "    ('groupby', FunctionTransformer(perform_groupby)),\n",
    "    ('reset_index', FunctionTransformer(reset_index_func)),\n",
    "    ('standardize_columns', FunctionTransformer(standardize_columns, kw_args={'columns_to_standardize': ['lamax', 'laeq']})),\n",
    "    ('drop_columns', FunctionTransformer(drop_columns)),\n",
    "    ('date_transformer', DateTransformer()),\n",
    "    ('drop_year_column', FunctionTransformer(drop_year_column))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monthly pipeline\n",
    "# Step 6: Perform groupby to create dataframe per hour\n",
    "def perform_groupby(df):\n",
    "    return df.groupby(['month', 'description']).mean()\n",
    "\n",
    "# Step 7: Reset index\n",
    "def reset_index_func(df):\n",
    "    return df.reset_index()\n",
    "\n",
    "# Step 8: Standardize the data\n",
    "def standardize_columns(df, columns_to_standardize):\n",
    "    scaler = StandardScaler()\n",
    "    standardized_values = scaler.fit_transform(df[columns_to_standardize])\n",
    "    new_columns = [column + '_standardized' for column in columns_to_standardize]\n",
    "    df[new_columns] = pd.DataFrame(standardized_values, columns=new_columns)\n",
    "    return df\n",
    "\n",
    "# Step 9: Drop unwanted columns\n",
    "def drop_columns(df):\n",
    "    columns_to_drop = ['day', 'hour']\n",
    "    return df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 10: Define a custom transformer to create the new column\n",
    "class DateTransformer:\n",
    "    def transform(self, df):\n",
    "        df['year'] = 2022\n",
    "        df['date'] = df.apply(lambda row: pd.to_datetime(f\"{int(row['month']):02d}-{int(row['year']):04d}\", format='%m-%Y'), axis=1)\n",
    "        df['date'] = df['date'].dt.strftime('%b %Y')\n",
    "        return df\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "    \n",
    "# Step 11: Drop the year column\n",
    "def drop_year_column(df):\n",
    "    return df.drop(columns='year')\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_monthly = Pipeline([\n",
    "    ('groupby', FunctionTransformer(perform_groupby)),\n",
    "    ('reset_index', FunctionTransformer(reset_index_func)),\n",
    "    ('standardize_columns', FunctionTransformer(standardize_columns, kw_args={'columns_to_standardize': ['lamax', 'laeq']})),\n",
    "    ('drop_columns', FunctionTransformer(drop_columns)),\n",
    "    ('date_transformer', DateTransformer()),\n",
    "    ('drop_year_column', FunctionTransformer(drop_year_column))\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- January "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs\n",
    "urls_jan = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_jan = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_jan in urls_jan:\n",
    "    df_jan = pd.read_csv(url_jan, header=0, sep=';')\n",
    "    dfs_jan.append(df_jan)\n",
    "\n",
    "# Now we have a list of DataFrames for each URL called dfs_jan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_feb = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_feb = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_feb in urls_feb:\n",
    "    df_feb = pd.read_csv(url_feb, header=0, sep=';')\n",
    "    dfs_feb.append(df_feb)\n",
    "\n",
    "# Now we have a list of DataFrames for each URL called dfs_feb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_mar = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_mar = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_mar in urls_mar:\n",
    "    df_mar = pd.read_csv(url_mar, header=0, sep=';')\n",
    "    dfs_mar.append(df_mar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- April"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_apr = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_apr = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_apr in urls_apr:\n",
    "    df_apr = pd.read_csv(url_apr, header=0, sep=';')\n",
    "    dfs_apr.append(df_apr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_may = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_may = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_may in urls_may:\n",
    "    df_may = pd.read_csv(url_may, header=0, sep=';')\n",
    "    dfs_may.append(df_may)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "url_jun = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_jun = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_jun in url_jun:\n",
    "    df_jun = pd.read_csv(url_jun, header=0, sep=';')\n",
    "    dfs_jun.append(df_jun)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_jul = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_jul = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_jul in urls_jul:\n",
    "    df_jul = pd.read_csv(url_jul, header=0, sep=';')\n",
    "    dfs_jul.append(df_jul)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_aug = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_aug = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_aug in urls_aug:\n",
    "    df_aug = pd.read_csv(url_aug, header=0, sep=';')\n",
    "    dfs_aug.append(df_aug)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- September"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_sep = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_sep = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_sep in urls_sep:\n",
    "    df_sep = pd.read_csv(url_sep, header=0, sep=';')\n",
    "    dfs_sep.append(df_sep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_oct = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_oct = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_oct in urls_oct:\n",
    "    df_oct = pd.read_csv(url_oct, header=0, sep=';')\n",
    "    dfs_oct.append(df_oct)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_nov = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_nov = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_nov in urls_nov:\n",
    "    df_nov = pd.read_csv(url_nov, header=0, sep=';')\n",
    "    dfs_nov.append(df_nov)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs \n",
    "urls_dec = [\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255439_mp-01-naamsestraat-35-maxim.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255440_mp-02-naamsestraat-57-xior.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255441_mp-03-naamsestraat-62-taste.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255444_mp-07-naamsestraat-81.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255445_mp-08-kiosk-stadspark.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_280324_mp08bis---vrijthof.csv',\n",
    "    'https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_303910_mp-04-his-hears.csv'\n",
    "   ]\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_dec = []\n",
    "\n",
    "# Loop through each URL and read the CSV into a DataFrame\n",
    "for url_dec in urls_dec:\n",
    "    df_dec = pd.read_csv(url_dec, header=0, sep=';')\n",
    "    dfs_dec.append(df_dec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets\n",
    "dfs_2022 = [dfs_jan,dfs_feb,dfs_mar,dfs_apr,dfs_may,dfs_jun,dfs_jul,dfs_aug,dfs_sep,dfs_oct,dfs_nov,dfs_dec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### in case kernal crashes again\n",
    "jan = pd.concat(dfs_jan, ignore_index=True)\n",
    "feb = pd.concat(dfs_feb, ignore_index=True)\n",
    "mar = pd.concat(dfs_mar, ignore_index=True)\n",
    "apr = pd.concat(dfs_apr, ignore_index=True)\n",
    "may = pd.concat(dfs_may, ignore_index=True)\n",
    "jun = pd.concat(dfs_jun, ignore_index=True)\n",
    "jul = pd.concat(dfs_jul, ignore_index=True)\n",
    "aug = pd.concat(dfs_aug, ignore_index=True)\n",
    "sep = pd.concat(dfs_sep, ignore_index=True)\n",
    "oct = pd.concat(dfs_oct, ignore_index=True)\n",
    "nov = pd.concat(dfs_nov, ignore_index=True)\n",
    "dec = pd.concat(dfs_dec, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "dfs_noise = [jan,feb,mar,apr,may,jun,jul,aug,sep,oct,nov,dec]\n",
    "combined_noise = pd.concat(dfs_noise, ignore_index=True)\n",
    "combined_noise.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## Create a full dataframe for later use\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs_noise = []\n",
    "\n",
    "# loop through each list and oncatenate noise data in each month\n",
    "for df_2022 in dfs_2022:\n",
    "    df_noise = pd.concat(df_2022, ignore_index=True)\n",
    "    dfs_noise.append(df_noise)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "combined_noise = pd.concat(dfs_noise, ignore_index=True)\n",
    "combined_noise.head(100000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To get data for modelling\n",
    "data_modelling = pipeline_modelling(combined_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>lamax</th>\n",
       "      <th>laeq</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>87.6</td>\n",
       "      <td>82.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>84.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>84.8</td>\n",
       "      <td>82.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>81.9</td>\n",
       "      <td>79.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>78.3</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>45.9</td>\n",
       "      <td>44.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>46.2</td>\n",
       "      <td>45.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>46.0</td>\n",
       "      <td>44.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>45.5</td>\n",
       "      <td>44.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>46.6</td>\n",
       "      <td>45.1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        description  lamax  laeq  month  day  hour\n",
       "0      MP 03: Naamsestraat 62 Taste   87.6  82.7      1    1     0\n",
       "1      MP 03: Naamsestraat 62 Taste   84.5  83.1      1    1     0\n",
       "2      MP 03: Naamsestraat 62 Taste   84.8  82.7      1    1     0\n",
       "3      MP 03: Naamsestraat 62 Taste   81.9  79.3      1    1     0\n",
       "4      MP 03: Naamsestraat 62 Taste   78.3  76.0      1    1     0\n",
       "...                             ...    ...   ...    ...  ...   ...\n",
       "99995  MP 03: Naamsestraat 62 Taste   45.9  44.7      1    2     3\n",
       "99996  MP 03: Naamsestraat 62 Taste   46.2  45.4      1    2     3\n",
       "99997  MP 03: Naamsestraat 62 Taste   46.0  44.6      1    2     3\n",
       "99998  MP 03: Naamsestraat 62 Taste   45.5  44.7      1    2     3\n",
       "99999  MP 03: Naamsestraat 62 Taste   46.6  45.1      1    2     3\n",
       "\n",
       "[100000 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To get hourly, daily, monthly data\n",
    "# Apply the pipeline to the overall dataset\n",
    "transformed_overall_datasets = []\n",
    "for df_2022 in dfs_2022:\n",
    "    transformed_overall_dataset = pipeline_general.fit_transform(df_2022)\n",
    "    transformed_overall_datasets.append(transformed_overall_dataset)\n",
    "\n",
    "combined = pd.concat(transformed_overall_datasets, ignore_index=True)\n",
    "combined.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>description</th>\n",
       "      <th>lamax</th>\n",
       "      <th>laeq</th>\n",
       "      <th>lamax_standardized</th>\n",
       "      <th>laeq_standardized</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>60.322528</td>\n",
       "      <td>57.126833</td>\n",
       "      <td>1.248969</td>\n",
       "      <td>1.044063</td>\n",
       "      <td>00:00 01-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>MP 05: Calvariekapel KU Leuven</td>\n",
       "      <td>53.230972</td>\n",
       "      <td>49.987639</td>\n",
       "      <td>0.114661</td>\n",
       "      <td>-0.103638</td>\n",
       "      <td>00:00 01-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>MP 06: Parkstraat 2 La Filosovia</td>\n",
       "      <td>53.666056</td>\n",
       "      <td>50.752000</td>\n",
       "      <td>0.184253</td>\n",
       "      <td>0.019241</td>\n",
       "      <td>00:00 01-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>MP 07: Naamsestraat 81</td>\n",
       "      <td>50.056861</td>\n",
       "      <td>47.440222</td>\n",
       "      <td>-0.393044</td>\n",
       "      <td>-0.513162</td>\n",
       "      <td>00:00 01-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>53.033583</td>\n",
       "      <td>50.853806</td>\n",
       "      <td>0.083088</td>\n",
       "      <td>0.035608</td>\n",
       "      <td>01:00 01-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55464</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>MP 04: His &amp; Hears</td>\n",
       "      <td>61.517005</td>\n",
       "      <td>58.995888</td>\n",
       "      <td>1.440028</td>\n",
       "      <td>1.344533</td>\n",
       "      <td>23:00 31-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55465</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>MP 05: Calvariekapel KU Leuven</td>\n",
       "      <td>59.659572</td>\n",
       "      <td>56.671964</td>\n",
       "      <td>1.142928</td>\n",
       "      <td>0.970938</td>\n",
       "      <td>23:00 31-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55466</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>MP 06: Parkstraat 2 La Filosovia</td>\n",
       "      <td>57.888775</td>\n",
       "      <td>55.366713</td>\n",
       "      <td>0.859685</td>\n",
       "      <td>0.761105</td>\n",
       "      <td>23:00 31-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55467</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>MP 07: Naamsestraat 81</td>\n",
       "      <td>55.636732</td>\n",
       "      <td>53.113476</td>\n",
       "      <td>0.499467</td>\n",
       "      <td>0.398874</td>\n",
       "      <td>23:00 31-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55468</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>MP08bis - Vrijthof</td>\n",
       "      <td>59.871492</td>\n",
       "      <td>58.237177</td>\n",
       "      <td>1.176825</td>\n",
       "      <td>1.222563</td>\n",
       "      <td>23:00 31-12-2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55469 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       month  day  hour                       description      lamax  \\\n",
       "0          1    1     0      MP 03: Naamsestraat 62 Taste  60.322528   \n",
       "1          1    1     0    MP 05: Calvariekapel KU Leuven  53.230972   \n",
       "2          1    1     0  MP 06: Parkstraat 2 La Filosovia  53.666056   \n",
       "3          1    1     0            MP 07: Naamsestraat 81  50.056861   \n",
       "4          1    1     1      MP 03: Naamsestraat 62 Taste  53.033583   \n",
       "...      ...  ...   ...                               ...        ...   \n",
       "55464     12   31    23                MP 04: His & Hears  61.517005   \n",
       "55465     12   31    23    MP 05: Calvariekapel KU Leuven  59.659572   \n",
       "55466     12   31    23  MP 06: Parkstraat 2 La Filosovia  57.888775   \n",
       "55467     12   31    23            MP 07: Naamsestraat 81  55.636732   \n",
       "55468     12   31    23                MP08bis - Vrijthof  59.871492   \n",
       "\n",
       "            laeq  lamax_standardized  laeq_standardized              date  \n",
       "0      57.126833            1.248969           1.044063  00:00 01-01-2022  \n",
       "1      49.987639            0.114661          -0.103638  00:00 01-01-2022  \n",
       "2      50.752000            0.184253           0.019241  00:00 01-01-2022  \n",
       "3      47.440222           -0.393044          -0.513162  00:00 01-01-2022  \n",
       "4      50.853806            0.083088           0.035608  01:00 01-01-2022  \n",
       "...          ...                 ...                ...               ...  \n",
       "55464  58.995888            1.440028           1.344533  23:00 31-12-2022  \n",
       "55465  56.671964            1.142928           0.970938  23:00 31-12-2022  \n",
       "55466  55.366713            0.859685           0.761105  23:00 31-12-2022  \n",
       "55467  53.113476            0.499467           0.398874  23:00 31-12-2022  \n",
       "55468  58.237177            1.176825           1.222563  23:00 31-12-2022  \n",
       "\n",
       "[55469 rows x 9 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the pipeline to the combined and collect hourly data\n",
    "combined_hourly = pipeline_hourly.fit_transform(combined)\n",
    "\n",
    "combined_hourly.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>description</th>\n",
       "      <th>lamax</th>\n",
       "      <th>laeq</th>\n",
       "      <th>lamax_standardized</th>\n",
       "      <th>laeq_standardized</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>51.665242</td>\n",
       "      <td>49.992637</td>\n",
       "      <td>-0.247251</td>\n",
       "      <td>-0.193655</td>\n",
       "      <td>01-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MP 05: Calvariekapel KU Leuven</td>\n",
       "      <td>48.747476</td>\n",
       "      <td>46.504067</td>\n",
       "      <td>-1.093428</td>\n",
       "      <td>-1.245260</td>\n",
       "      <td>01-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MP 06: Parkstraat 2 La Filosovia</td>\n",
       "      <td>48.270005</td>\n",
       "      <td>46.007220</td>\n",
       "      <td>-1.231899</td>\n",
       "      <td>-1.395031</td>\n",
       "      <td>01-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MP 07: Naamsestraat 81</td>\n",
       "      <td>45.908501</td>\n",
       "      <td>44.373056</td>\n",
       "      <td>-1.916755</td>\n",
       "      <td>-1.887638</td>\n",
       "      <td>01-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>51.407297</td>\n",
       "      <td>50.094018</td>\n",
       "      <td>-0.322058</td>\n",
       "      <td>-0.163094</td>\n",
       "      <td>02-01-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>MP 04: His &amp; Hears</td>\n",
       "      <td>55.134973</td>\n",
       "      <td>53.632457</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.903544</td>\n",
       "      <td>31-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>MP 05: Calvariekapel KU Leuven</td>\n",
       "      <td>51.925297</td>\n",
       "      <td>50.133166</td>\n",
       "      <td>-0.171833</td>\n",
       "      <td>-0.151293</td>\n",
       "      <td>31-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>MP 06: Parkstraat 2 La Filosovia</td>\n",
       "      <td>50.342353</td>\n",
       "      <td>48.683298</td>\n",
       "      <td>-0.630900</td>\n",
       "      <td>-0.588346</td>\n",
       "      <td>31-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>MP 07: Naamsestraat 81</td>\n",
       "      <td>49.665543</td>\n",
       "      <td>47.970367</td>\n",
       "      <td>-0.827181</td>\n",
       "      <td>-0.803254</td>\n",
       "      <td>31-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>MP08bis - Vrijthof</td>\n",
       "      <td>51.474387</td>\n",
       "      <td>49.131197</td>\n",
       "      <td>-0.302601</td>\n",
       "      <td>-0.453330</td>\n",
       "      <td>31-12-2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2322 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  day                       description      lamax       laeq  \\\n",
       "0         1    1      MP 03: Naamsestraat 62 Taste  51.665242  49.992637   \n",
       "1         1    1    MP 05: Calvariekapel KU Leuven  48.747476  46.504067   \n",
       "2         1    1  MP 06: Parkstraat 2 La Filosovia  48.270005  46.007220   \n",
       "3         1    1            MP 07: Naamsestraat 81  45.908501  44.373056   \n",
       "4         1    2      MP 03: Naamsestraat 62 Taste  51.407297  50.094018   \n",
       "...     ...  ...                               ...        ...        ...   \n",
       "2317     12   31                MP 04: His & Hears  55.134973  53.632457   \n",
       "2318     12   31    MP 05: Calvariekapel KU Leuven  51.925297  50.133166   \n",
       "2319     12   31  MP 06: Parkstraat 2 La Filosovia  50.342353  48.683298   \n",
       "2320     12   31            MP 07: Naamsestraat 81  49.665543  47.970367   \n",
       "2321     12   31                MP08bis - Vrijthof  51.474387  49.131197   \n",
       "\n",
       "      lamax_standardized  laeq_standardized        date  \n",
       "0              -0.247251          -0.193655  01-01-2022  \n",
       "1              -1.093428          -1.245260  01-01-2022  \n",
       "2              -1.231899          -1.395031  01-01-2022  \n",
       "3              -1.916755          -1.887638  01-01-2022  \n",
       "4              -0.322058          -0.163094  02-01-2022  \n",
       "...                  ...                ...         ...  \n",
       "2317            0.759000           0.903544  31-12-2022  \n",
       "2318           -0.171833          -0.151293  31-12-2022  \n",
       "2319           -0.630900          -0.588346  31-12-2022  \n",
       "2320           -0.827181          -0.803254  31-12-2022  \n",
       "2321           -0.302601          -0.453330  31-12-2022  \n",
       "\n",
       "[2322 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the pipeline to the combined and collect daily data\n",
    "combined_daily = pipeline_daily.fit_transform(combined)\n",
    "\n",
    "combined_daily.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>description</th>\n",
       "      <th>lamax</th>\n",
       "      <th>laeq</th>\n",
       "      <th>lamax_standardized</th>\n",
       "      <th>laeq_standardized</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MP 03: Naamsestraat 62 Taste</td>\n",
       "      <td>53.239009</td>\n",
       "      <td>51.727544</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>0.391739</td>\n",
       "      <td>Jan 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MP 05: Calvariekapel KU Leuven</td>\n",
       "      <td>50.374792</td>\n",
       "      <td>48.560197</td>\n",
       "      <td>-0.780260</td>\n",
       "      <td>-0.796668</td>\n",
       "      <td>Jan 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>MP 06: Parkstraat 2 La Filosovia</td>\n",
       "      <td>50.086348</td>\n",
       "      <td>48.274795</td>\n",
       "      <td>-0.882857</td>\n",
       "      <td>-0.903753</td>\n",
       "      <td>Jan 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>MP 07: Naamsestraat 81</td>\n",
       "      <td>48.800443</td>\n",
       "      <td>47.515371</td>\n",
       "      <td>-1.340242</td>\n",
       "      <td>-1.188693</td>\n",
       "      <td>Jan 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>57.287668</td>\n",
       "      <td>55.245502</td>\n",
       "      <td>1.678588</td>\n",
       "      <td>1.711697</td>\n",
       "      <td>Feb 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>12</td>\n",
       "      <td>MP 04: His &amp; Hears</td>\n",
       "      <td>54.776651</td>\n",
       "      <td>53.133224</td>\n",
       "      <td>0.785442</td>\n",
       "      <td>0.919158</td>\n",
       "      <td>Dec 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>12</td>\n",
       "      <td>MP 05: Calvariekapel KU Leuven</td>\n",
       "      <td>52.407326</td>\n",
       "      <td>50.394638</td>\n",
       "      <td>-0.057306</td>\n",
       "      <td>-0.108375</td>\n",
       "      <td>Dec 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>12</td>\n",
       "      <td>MP 06: Parkstraat 2 La Filosovia</td>\n",
       "      <td>51.694696</td>\n",
       "      <td>49.915926</td>\n",
       "      <td>-0.310782</td>\n",
       "      <td>-0.287991</td>\n",
       "      <td>Dec 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>12</td>\n",
       "      <td>MP 07: Naamsestraat 81</td>\n",
       "      <td>50.699741</td>\n",
       "      <td>49.103125</td>\n",
       "      <td>-0.664678</td>\n",
       "      <td>-0.592958</td>\n",
       "      <td>Dec 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>12</td>\n",
       "      <td>MP08bis - Vrijthof</td>\n",
       "      <td>46.274635</td>\n",
       "      <td>44.514053</td>\n",
       "      <td>-2.238649</td>\n",
       "      <td>-2.314805</td>\n",
       "      <td>Dec 2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    month                       description      lamax       laeq  \\\n",
       "0       1      MP 03: Naamsestraat 62 Taste  53.239009  51.727544   \n",
       "1       1    MP 05: Calvariekapel KU Leuven  50.374792  48.560197   \n",
       "2       1  MP 06: Parkstraat 2 La Filosovia  50.086348  48.274795   \n",
       "3       1            MP 07: Naamsestraat 81  48.800443  47.515371   \n",
       "4       2     MP 01: Naamsestraat 35  Maxim  57.287668  55.245502   \n",
       "..    ...                               ...        ...        ...   \n",
       "77     12                MP 04: His & Hears  54.776651  53.133224   \n",
       "78     12    MP 05: Calvariekapel KU Leuven  52.407326  50.394638   \n",
       "79     12  MP 06: Parkstraat 2 La Filosovia  51.694696  49.915926   \n",
       "80     12            MP 07: Naamsestraat 81  50.699741  49.103125   \n",
       "81     12                MP08bis - Vrijthof  46.274635  44.514053   \n",
       "\n",
       "    lamax_standardized  laeq_standardized      date  \n",
       "0             0.238516           0.391739  Jan 2022  \n",
       "1            -0.780260          -0.796668  Jan 2022  \n",
       "2            -0.882857          -0.903753  Jan 2022  \n",
       "3            -1.340242          -1.188693  Jan 2022  \n",
       "4             1.678588           1.711697  Feb 2022  \n",
       "..                 ...                ...       ...  \n",
       "77            0.785442           0.919158  Dec 2022  \n",
       "78           -0.057306          -0.108375  Dec 2022  \n",
       "79           -0.310782          -0.287991  Dec 2022  \n",
       "80           -0.664678          -0.592958  Dec 2022  \n",
       "81           -2.238649          -2.314805  Dec 2022  \n",
       "\n",
       "[82 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the pipeline to the combined and collect monthly data\n",
    "combined_monthly = pipeline_monthly.fit_transform(combined)\n",
    "\n",
    "combined_monthly.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month                 0\n",
      "day                   0\n",
      "hour                  0\n",
      "description           0\n",
      "lamax                 0\n",
      "laeq                  0\n",
      "lamax_standardized    0\n",
      "laeq_standardized     0\n",
      "date                  0\n",
      "dtype: int64\n",
      "month                 0\n",
      "day                   0\n",
      "description           0\n",
      "lamax                 0\n",
      "laeq                  0\n",
      "lamax_standardized    0\n",
      "laeq_standardized     0\n",
      "date                  0\n",
      "dtype: int64\n",
      "month                 0\n",
      "description           0\n",
      "lamax                 0\n",
      "laeq                  0\n",
      "lamax_standardized    0\n",
      "laeq_standardized     0\n",
      "date                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check whether there are missing values left\n",
    "print(combined_hourly.isnull().sum())\n",
    "print(combined_daily.isnull().sum())\n",
    "print(combined_monthly.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting file (only needs to be run one time so comment it out)\n",
    "combined_hourly.to_csv('hourly_noisedata_2022.csv', index=False)  \n",
    "combined_daily.to_csv('daily_noisedata_2022.csv', index=False) \n",
    "combined_monthly.to_csv('monthly_noisedata_2022.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to get modelling dataset\n",
    "data_modelling = pipeline_modelling.fit_transform(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_hourly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4l/srd4ffps1qz5k88lkjbdywqw0000gn/T/ipykernel_84681/3228448897.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Delete the separate dataframes to minimize memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mcombined_hourly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mcombined_daily\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mcombined_monthly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_hourly' is not defined"
     ]
    }
   ],
   "source": [
    "# Delete the separate dataframes to minimize memory usage\n",
    "del combined_hourly\n",
    "del combined_daily\n",
    "del combined_monthly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the events data\n",
    "event_mp01 = pd.read_csv('/Users/tianying/Documents/Statistics and Data Science/Course/Modern Data Analytics/MDA project/MDA-Georgia/Data/csv_results_41_255439_mp-01-naamsestraat-35-maxim.csv',delimiter=';')\n",
    "event_mp02 = pd.read_csv('/Users/tianying/Documents/Statistics and Data Science/Course/Modern Data Analytics/MDA project/MDA-Georgia/Data/csv_results_41_255440_mp-02-naamsestraat-57-xior.csv',delimiter=';')\n",
    "event_mp03 = pd.read_csv('/Users/tianying/Documents/Statistics and Data Science/Course/Modern Data Analytics/MDA project/MDA-Georgia/Data/csv_results_41_255441_mp-03-naamsestraat-62-taste.csv',delimiter=';')\n",
    "event_mp04 = pd.read_csv('/Users/tianying/Documents/Statistics and Data Science/Course/Modern Data Analytics/MDA project/MDA-Georgia/Data/csv_results_41_303910_mp-04-his-hears.csv',delimiter=';')\n",
    "event_mp05 = pd.read_csv('/Users/tianying/Documents/Statistics and Data Science/Course/Modern Data Analytics/MDA project/MDA-Georgia/Data/csv_results_41_255442_mp-05-calvariekapel-ku-leuven.csv',delimiter=';')\n",
    "event_mp06 = pd.read_csv('/Users/tianying/Documents/Statistics and Data Science/Course/Modern Data Analytics/MDA project/MDA-Georgia/Data/csv_results_41_255443_mp-06-parkstraat-2-la-filosovia.csv',delimiter=';')\n",
    "event_mp07 = pd.read_csv('/Users/tianying/Documents/Statistics and Data Science/Course/Modern Data Analytics/MDA project/MDA-Georgia/Data/csv_results_41_255444_mp-07-naamsestraat-81.csv',delimiter=';')\n",
    "event_mp08stadspark = pd.read_csv('/Users/tianying/Documents/Statistics and Data Science/Course/Modern Data Analytics/MDA project/MDA-Georgia/Data/csv_results_41_255445_mp-08-kiosk-stadspark.csv',delimiter=';')\n",
    "event_mp08Vrijthof = pd.read_csv('/Users/tianying/Documents/Statistics and Data Science/Course/Modern Data Analytics/MDA project/MDA-Georgia/Data/csv_results_41_280324_mp08bis---vrijthof.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the events data\n",
    "events = [event_mp01,event_mp02,event_mp03,event_mp04,event_mp05,event_mp06,event_mp07,event_mp08Vrijthof]\n",
    "combined_event = pd.concat(events, ignore_index=True)\n",
    "\n",
    "combined_event.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the event data with noise data\n",
    "merged_noise_event = pd.merge(combined_noise, combined_event, on=['description','result_timestamp'],  how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the datasets for January\n",
    "#combined_jan = pd.concat(dfs, ignore_index=True)\n",
    "#print(combined_jan.head())\n",
    "\n",
    "#del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "#combined_jan['result_timestamp'] = pd.to_datetime(combined_jan['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "#combined_jan['month'] = combined_jan['result_timestamp'].dt.month\n",
    "#combined_jan['day'] = combined_jan['result_timestamp'].dt.day\n",
    "#combined_jan['hour'] = combined_jan['result_timestamp'].dt.hour\n",
    "#combined_jan['minute'] = combined_jan['result_timestamp'].dt.minute\n",
    "\n",
    "#combined_jan.head()\n",
    "\n",
    "\n",
    "# Drop the columns we won't use\n",
    "#columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "#columns_to_drop = set(combined_jan.columns) - set(columns_to_keep)\n",
    "#combined_jan.drop(columns=columns_to_drop, inplace=True)\n",
    "#combined_jan.head()\n",
    "\n",
    "\n",
    "# check for missing values in each column\n",
    "#print(combined_jan.isnull().sum())\n",
    "\n",
    "\n",
    "# forward fill missing values \n",
    "#combined_jan.ffill(inplace=True)\n",
    "\n",
    "# check whether there are missing values left\n",
    "#print(combined_jan.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "#jan_per_hour = combined_jan.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "#jan_per_hour = jan_per_hour.reset_index()\n",
    "#jan_per_hour.head()\n",
    "\n",
    "\n",
    "# Create dataframe per day\n",
    "#combined_jan.drop('hour', axis=1, inplace=True)\n",
    "#jan_per_day= combined_jan.groupby(['month', 'day', 'description']).mean()\n",
    "#jan_per_day = jan_per_day.reset_index()\n",
    "#jan_per_day.head()\n",
    "\n",
    "\n",
    "# Create dataframe per month\n",
    "#combined_jan.drop('day', axis=1, inplace=True)\n",
    "#jan_per_month = combined_jan.groupby(['month', 'description']).mean()\n",
    "#jan_per_month = jan_per_month.reset_index()\n",
    "#jan_per_month.head()\n",
    "\n",
    "\n",
    "#del combined_jan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the datasets for February\n",
    "#combined_feb = pd.concat(dfs, ignore_index=True)\n",
    "#del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "#combined_feb['result_timestamp'] = pd.to_datetime(combined_feb['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "#combined_feb['month'] = combined_feb['result_timestamp'].dt.month\n",
    "#combined_feb['day'] = combined_feb['result_timestamp'].dt.day\n",
    "#combined_feb['hour'] = combined_feb['result_timestamp'].dt.hour\n",
    "#combined_feb['minute'] = combined_feb['result_timestamp'].dt.minute\n",
    "\n",
    "#combined_feb.head()\n",
    "\n",
    "\n",
    "# Drop the columns we won't use\n",
    "#columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "#columns_to_drop = set(combined_feb.columns) - set(columns_to_keep)\n",
    "#combined_feb.drop(columns=columns_to_drop, inplace=True)\n",
    "#combined_feb.head()\n",
    "\n",
    "\n",
    "# check for missing values in each column\n",
    "#print(combined_feb.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "#feb_per_hour = combined_feb.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "#feb_per_hour = feb_per_hour.reset_index()\n",
    "#print(feb_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "#combined_feb.drop('hour', axis=1, inplace=True)\n",
    "#feb_per_day = combined_feb.groupby(['month', 'day', 'description']).mean()\n",
    "#feb_per_day = feb_per_day.reset_index()\n",
    "#print(feb_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "#combined_feb.drop('day', axis=1, inplace=True)\n",
    "#feb_per_month = combined_feb.groupby(['month', 'description']).mean()\n",
    "#feb_per_month = feb_per_month.reset_index()\n",
    "#print(feb_per_month.head())\n",
    "\n",
    "#del combined_feb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#object_id</th>\n",
       "      <th>description</th>\n",
       "      <th>result_timestamp</th>\n",
       "      <th>lamax</th>\n",
       "      <th>lamax_unit</th>\n",
       "      <th>laeq</th>\n",
       "      <th>laeq_unit</th>\n",
       "      <th>lceq</th>\n",
       "      <th>lceq_unit</th>\n",
       "      <th>lcpeak</th>\n",
       "      <th>lcpeak_unit</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-03-01 00:00:00.462</td>\n",
       "      <td>60.5</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>57.9</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>63.36</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>76.57</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-03-01 00:00:01.462</td>\n",
       "      <td>54.1</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>53.2</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>61.86</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>74.52</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-03-01 00:00:02.462</td>\n",
       "      <td>61.4</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>57.5</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>64.12</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>76.46</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-03-01 00:00:03.462</td>\n",
       "      <td>61.3</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>59.1</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>64.68</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>76.67</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-03-01 00:00:04.462</td>\n",
       "      <td>61.1</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>58.4</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>64.53</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>77.07</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  #object_id                    description        result_timestamp  lamax  \\\n",
       "0     255439  MP 01: Naamsestraat 35  Maxim 2022-03-01 00:00:00.462   60.5   \n",
       "1     255439  MP 01: Naamsestraat 35  Maxim 2022-03-01 00:00:01.462   54.1   \n",
       "2     255439  MP 01: Naamsestraat 35  Maxim 2022-03-01 00:00:02.462   61.4   \n",
       "3     255439  MP 01: Naamsestraat 35  Maxim 2022-03-01 00:00:03.462   61.3   \n",
       "4     255439  MP 01: Naamsestraat 35  Maxim 2022-03-01 00:00:04.462   61.1   \n",
       "\n",
       "  lamax_unit  laeq laeq_unit   lceq lceq_unit  lcpeak lcpeak_unit  month  day  \\\n",
       "0      dB(A)  57.9     dB(A)  63.36     dB(C)   76.57       dB(C)      3    1   \n",
       "1      dB(A)  53.2     dB(A)  61.86     dB(C)   74.52       dB(C)      3    1   \n",
       "2      dB(A)  57.5     dB(A)  64.12     dB(C)   76.46       dB(C)      3    1   \n",
       "3      dB(A)  59.1     dB(A)  64.68     dB(C)   76.67       dB(C)      3    1   \n",
       "4      dB(A)  58.4     dB(A)  64.53     dB(C)   77.07       dB(C)      3    1   \n",
       "\n",
       "   hour  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the datasets for March\n",
    "#combined_mar = pd.concat(dfs, ignore_index=True)\n",
    "#del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "#combined_mar['result_timestamp'] = pd.to_datetime(combined_mar['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "#combined_mar['month'] = combined_mar['result_timestamp'].dt.month\n",
    "#combined_mar['day'] = combined_mar['result_timestamp'].dt.day\n",
    "#combined_mar['hour'] = combined_mar['result_timestamp'].dt.hour\n",
    "#combined_mar['minute'] = combined_mar['result_timestamp'].dt.minute\n",
    "\n",
    "#combined_mar.head()\n",
    "\n",
    "\n",
    "# Drop the columns we won't use\n",
    "#columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "#columns_to_drop = set(combined_mar.columns) - set(columns_to_keep)\n",
    "#combined_mar.drop(columns=columns_to_drop, inplace=True)\n",
    "#combined_mar.head()\n",
    "\n",
    "\n",
    "# check for missing values in each column\n",
    "#print(combined_mar.isnull().sum())\n",
    "\n",
    "\n",
    "# forward fill missing values \n",
    "#combined_mar.ffill(inplace=True)\n",
    "\n",
    "# check whether there are missing values left\n",
    "#print(combined_mar.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "#mar_per_hour = combined_mar.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "#mar_per_hour = mar_per_hour.reset_index()\n",
    "#print(mar_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "#combined_mar.drop('hour', axis=1, inplace=True)\n",
    "#mar_per_day = combined_mar.groupby(['month', 'day', 'description']).mean()\n",
    "#mar_per_day = mar_per_day.reset_index()\n",
    "#print(mar_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "#combined_mar.drop('day', axis=1, inplace=True)\n",
    "#mar_per_month = combined_mar.groupby(['month', 'description']).mean()\n",
    "#mar_per_month = mar_per_month.reset_index()\n",
    "#print(mar_per_month.head())\n",
    "\n",
    "#del combined_mar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- April"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#object_id</th>\n",
       "      <th>description</th>\n",
       "      <th>result_timestamp</th>\n",
       "      <th>lamax</th>\n",
       "      <th>lamax_unit</th>\n",
       "      <th>laeq</th>\n",
       "      <th>laeq_unit</th>\n",
       "      <th>lceq</th>\n",
       "      <th>lceq_unit</th>\n",
       "      <th>lcpeak</th>\n",
       "      <th>lcpeak_unit</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-04-01 00:00:00.520</td>\n",
       "      <td>62.5</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>59.3</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>64.56</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>78.13</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-04-01 00:00:01.520</td>\n",
       "      <td>63.3</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>61.3</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>65.35</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>78.09</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-04-01 00:00:02.520</td>\n",
       "      <td>61.4</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>59.1</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>64.18</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>76.63</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-04-01 00:00:03.510</td>\n",
       "      <td>58.9</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>56.6</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>63.58</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>75.74</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255439</td>\n",
       "      <td>MP 01: Naamsestraat 35  Maxim</td>\n",
       "      <td>2022-04-01 00:00:04.510</td>\n",
       "      <td>59.6</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>57.2</td>\n",
       "      <td>dB(A)</td>\n",
       "      <td>63.32</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>77.86</td>\n",
       "      <td>dB(C)</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  #object_id                    description        result_timestamp  lamax  \\\n",
       "0     255439  MP 01: Naamsestraat 35  Maxim 2022-04-01 00:00:00.520   62.5   \n",
       "1     255439  MP 01: Naamsestraat 35  Maxim 2022-04-01 00:00:01.520   63.3   \n",
       "2     255439  MP 01: Naamsestraat 35  Maxim 2022-04-01 00:00:02.520   61.4   \n",
       "3     255439  MP 01: Naamsestraat 35  Maxim 2022-04-01 00:00:03.510   58.9   \n",
       "4     255439  MP 01: Naamsestraat 35  Maxim 2022-04-01 00:00:04.510   59.6   \n",
       "\n",
       "  lamax_unit  laeq laeq_unit   lceq lceq_unit  lcpeak lcpeak_unit  month  day  \\\n",
       "0      dB(A)  59.3     dB(A)  64.56     dB(C)   78.13       dB(C)      4    1   \n",
       "1      dB(A)  61.3     dB(A)  65.35     dB(C)   78.09       dB(C)      4    1   \n",
       "2      dB(A)  59.1     dB(A)  64.18     dB(C)   76.63       dB(C)      4    1   \n",
       "3      dB(A)  56.6     dB(A)  63.58     dB(C)   75.74       dB(C)      4    1   \n",
       "4      dB(A)  57.2     dB(A)  63.32     dB(C)   77.86       dB(C)      4    1   \n",
       "\n",
       "   hour  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the datasets \n",
    "#combined_apr = pd.concat(dfs, ignore_index=True)\n",
    "#del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "#combined_apr['result_timestamp'] = pd.to_datetime(combined_apr['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "#combined_apr['month'] = combined_apr['result_timestamp'].dt.month\n",
    "#combined_apr['day'] = combined_apr['result_timestamp'].dt.day\n",
    "#combined_apr['hour'] = combined_apr['result_timestamp'].dt.hour\n",
    "#combined_apr['minute'] = combined_apr['result_timestamp'].dt.minute\n",
    "\n",
    "#combined_apr.head()\n",
    "\n",
    "\n",
    "# Drop the columns we won't use\n",
    "#columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "#columns_to_drop = set(combined_apr.columns) - set(columns_to_keep)\n",
    "#combined_apr.drop(columns=columns_to_drop, inplace=True)\n",
    "#combined_apr.head()\n",
    "\n",
    "\n",
    "# check for missing values in each column\n",
    "#print(combined_apr.isnull().sum() / len(combined_apr))\n",
    "\n",
    "\n",
    "# forward fill missing values \n",
    "#combined_apr.ffill(inplace=True)\n",
    "\n",
    "# check whether there are missing values left\n",
    "#print(combined_apr.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "#apr_per_hour = combined_apr.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "#apr_per_hour = apr_per_hour.reset_index()\n",
    "#print(apr_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "#combined_apr.drop('hour', axis=1, inplace=True)\n",
    "#apr_per_day = combined_apr.groupby(['month', 'day', 'description']).mean()\n",
    "#apr_per_day = apr_per_day.reset_index()\n",
    "#print(apr_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "#combined_apr.drop('day', axis=1, inplace=True)\n",
    "#apr_per_month = combined_apr.groupby(['month', 'description']).mean()\n",
    "#apr_per_month = apr_per_month.reset_index()\n",
    "#print(apr_per_month.head())\n",
    "\n",
    "#del combined_apr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description     0\n",
      "lamax          11\n",
      "laeq           11\n",
      "month           0\n",
      "day             0\n",
      "hour            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combining the datasets \n",
    "#combined_may = pd.concat(dfs, ignore_index=True)\n",
    "#del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "#combined_may['result_timestamp'] = pd.to_datetime(combined_may['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "#combined_may['month'] = combined_may['result_timestamp'].dt.month\n",
    "#combined_may['day'] = combined_may['result_timestamp'].dt.day\n",
    "#combined_may['hour'] = combined_may['result_timestamp'].dt.hour\n",
    "#combined_may['minute'] = combined_may['result_timestamp'].dt.minute\n",
    "\n",
    "# Drop the columns we won't use\n",
    "#columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "#columns_to_drop = set(combined_may.columns) - set(columns_to_keep)\n",
    "#combined_may.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# check for missing values in each column\n",
    "#print(combined_may.isnull().sum())\n",
    "\n",
    "\n",
    "# forward fill missing values \n",
    "#combined_may.ffill(inplace=True)\n",
    "\n",
    "# check whether there are missing values left\n",
    "#print(combined_may.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "#may_per_hour = combined_may.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "#may_per_hour = may_per_hour.reset_index()\n",
    "#print(may_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "#combined_may.drop('hour', axis=1, inplace=True)\n",
    "#may_per_day = combined_may.groupby(['month', 'day', 'description']).mean()\n",
    "#may_per_day = may_per_day.reset_index()\n",
    "#print(may_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "#combined_may.drop('day', axis=1, inplace=True)\n",
    "#may_per_month = combined_may.groupby(['month', 'description']).mean()\n",
    "#may_per_month = may_per_month.reset_index()\n",
    "#print(may_per_month.head())\n",
    "\n",
    "#del combined_may"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description    0\n",
      "lamax          0\n",
      "laeq           0\n",
      "month          0\n",
      "day            0\n",
      "hour           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combining the datasets \n",
    "#combined_jun = pd.concat(dfs, ignore_index=True)\n",
    "#del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "#combined_jun['result_timestamp'] = pd.to_datetime(combined_jun['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "#combined_jun['month'] = combined_jun['result_timestamp'].dt.month\n",
    "#combined_jun['day'] = combined_jun['result_timestamp'].dt.day\n",
    "#combined_jun['hour'] = combined_jun['result_timestamp'].dt.hour\n",
    "#combined_jun['minute'] = combined_jun['result_timestamp'].dt.minute\n",
    "\n",
    "# Drop the columns we won't use\n",
    "#columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "#columns_to_drop = set(combined_jun.columns) - set(columns_to_keep)\n",
    "#combined_jun.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# check for missing values in each column\n",
    "#print(combined_jun.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "#jun_per_hour = combined_jun.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "#jun_per_hour = jun_per_hour.reset_index()\n",
    "#print(jun_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "#combined_jun.drop('hour', axis=1, inplace=True)\n",
    "#jun_per_day = combined_jun.groupby(['month', 'day', 'description']).mean()\n",
    "#jun_per_day = jun_per_day.reset_index()\n",
    "#print(jun_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "#combined_jun.drop('day', axis=1, inplace=True)\n",
    "#jun_per_month = combined_jun.groupby(['month', 'description']).mean()\n",
    "#jun_per_month = jun_per_month.reset_index()\n",
    "#print(jun_per_month.head())\n",
    "\n",
    "#del combined_jun"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description    0\n",
      "lamax          5\n",
      "laeq           5\n",
      "month          0\n",
      "day            0\n",
      "hour           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combining the datasets for \n",
    "#combined_jul = pd.concat(dfs, ignore_index=True)\n",
    "#del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "#combined_jul['result_timestamp'] = pd.to_datetime(combined_jul['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "#combined_jul['month'] = combined_jul['result_timestamp'].dt.month\n",
    "#combined_jul['day'] = combined_jul['result_timestamp'].dt.day\n",
    "#combined_jul['hour'] = combined_jul['result_timestamp'].dt.hour\n",
    "#combined_jul['minute'] = combined_jul['result_timestamp'].dt.minute\n",
    "\n",
    "# Drop the columns we won't use\n",
    "#columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "#columns_to_drop = set(combined_jul.columns) - set(columns_to_keep)\n",
    "#combined_jul.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# check for missing values in each column\n",
    "#print(combined_jul.isnull().sum())\n",
    "\n",
    "\n",
    "# forward fill missing values \n",
    "#combined_jul.ffill(inplace=True)\n",
    "\n",
    "# check whether there are missing values left\n",
    "#print(combined_jul.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "#jul_per_hour = combined_jul.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "#jul_per_hour = jul_per_hour.reset_index()\n",
    "#print(jul_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "#combined_jul.drop('hour', axis=1, inplace=True)\n",
    "#jul_per_day = combined_jul.groupby(['month', 'day', 'description']).mean()\n",
    "#jul_per_day = jul_per_day.reset_index()\n",
    "#print(jul_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "#combined_jul.drop('day', axis=1, inplace=True)\n",
    "#jul_per_month = combined_jul.groupby(['month', 'description']).mean()\n",
    "#jul_per_month = jul_per_month.reset_index()\n",
    "#print(jul_per_month.head())\n",
    "\n",
    "#del combined_jul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description    0\n",
      "lamax          0\n",
      "laeq           0\n",
      "month          0\n",
      "day            0\n",
      "hour           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combining the datasets for\n",
    "#combined_aug = pd.concat(dfs, ignore_index=True)\n",
    "#del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "#combined_aug['result_timestamp'] = pd.to_datetime(combined_aug['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "#combined_aug['month'] = combined_aug['result_timestamp'].dt.month\n",
    "#combined_aug['day'] = combined_aug['result_timestamp'].dt.day\n",
    "#combined_aug['hour'] = combined_aug['result_timestamp'].dt.hour\n",
    "#combined_aug['minute'] = combined_aug['result_timestamp'].dt.minute\n",
    "\n",
    "# Drop the columns we won't use\n",
    "#columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "#columns_to_drop = set(combined_aug.columns) - set(columns_to_keep)\n",
    "#combined_aug.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# check for missing values in each column\n",
    "#print(combined_aug.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "#aug_per_hour = combined_aug.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "#aug_per_hour = aug_per_hour.reset_index()\n",
    "#print(aug_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "#combined_aug.drop('hour', axis=1, inplace=True)\n",
    "#aug_per_day = combined_aug.groupby(['month', 'day', 'description']).mean()\n",
    "#aug_per_day = aug_per_day.reset_index()\n",
    "#print(aug_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "#combined_aug.drop('day', axis=1, inplace=True)\n",
    "#aug_per_month = combined_aug.groupby(['month', 'description']).mean()\n",
    "#aug_per_month = aug_per_month.reset_index()\n",
    "#print(aug_per_month.head())\n",
    "\n",
    "#del combined_aug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- September"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description     0\n",
      "lamax           2\n",
      "laeq           12\n",
      "month           0\n",
      "day             0\n",
      "hour            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Combining the datasets \n",
    "combined_sep = pd.concat(dfs, ignore_index=True)\n",
    "del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "combined_sep['result_timestamp'] = pd.to_datetime(combined_sep['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "combined_sep['month'] = combined_sep['result_timestamp'].dt.month\n",
    "combined_sep['day'] = combined_sep['result_timestamp'].dt.day\n",
    "combined_sep['hour'] = combined_sep['result_timestamp'].dt.hour\n",
    "#combined_sep['minute'] = combined_sep['result_timestamp'].dt.minute\n",
    "\n",
    "# Drop the columns we won't use\n",
    "columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "columns_to_drop = set(combined_sep.columns) - set(columns_to_keep)\n",
    "combined_sep.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# check for missing values in each column\n",
    "print(combined_sep.isnull().sum())\n",
    "\n",
    "\n",
    "# forward fill missing values \n",
    "combined_sep.ffill(inplace=True)\n",
    "\n",
    "# check whether there are missing values left\n",
    "print(combined_sep.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "sep_per_hour = combined_sep.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "sep_per_hour = sep_per_hour.reset_index()\n",
    "print(sep_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "combined_sep.drop('hour', axis=1, inplace=True)\n",
    "sep_per_day = combined_sep.groupby(['month', 'day', 'description']).mean()\n",
    "sep_per_day = sep_per_day.reset_index()\n",
    "print(sep_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "combined_sep.drop('day', axis=1, inplace=True)\n",
    "sep_per_month = combined_sep.groupby(['month', 'description']).mean()\n",
    "sep_per_month = sep_per_month.reset_index()\n",
    "print(sep_per_month.head())\n",
    "\n",
    "del combined_sep\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description     0\n",
      "lamax          10\n",
      "laeq           10\n",
      "month           0\n",
      "day             0\n",
      "hour            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Combining the datasets \n",
    "combined_oct = pd.concat(dfs, ignore_index=True)\n",
    "del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "combined_oct['result_timestamp'] = pd.to_datetime(combined_oct['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "combined_oct['month'] = combined_oct['result_timestamp'].dt.month\n",
    "combined_oct['day'] = combined_oct['result_timestamp'].dt.day\n",
    "combined_oct['hour'] = combined_oct['result_timestamp'].dt.hour\n",
    "#combined_oct['minute'] = combined_oct['result_timestamp'].dt.minute\n",
    "\n",
    "# Drop the columns we won't use\n",
    "columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "columns_to_drop = set(combined_oct.columns) - set(columns_to_keep)\n",
    "combined_oct.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# check for missing values in each column\n",
    "print(combined_oct.isnull().sum())\n",
    "\n",
    "\n",
    "# forward fill missing values \n",
    "combined_oct.ffill(inplace=True)\n",
    "\n",
    "# check whether there are missing values left\n",
    "print(combined_oct.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "oct_per_hour = combined_oct.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "oct_per_hour = oct_per_hour.reset_index()\n",
    "print(oct_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "combined_oct.drop('hour', axis=1, inplace=True)\n",
    "oct_per_day = combined_oct.groupby(['month', 'day', 'description']).mean()\n",
    "oct_per_day = oct_per_day.reset_index()\n",
    "print(oct_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "combined_oct.drop('day', axis=1, inplace=True)\n",
    "oct_per_month = combined_oct.groupby(['month', 'description']).mean()\n",
    "oct_per_month = oct_per_month.reset_index()\n",
    "print(oct_per_month.head())\n",
    "\n",
    "del combined_oct\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description    0\n",
      "lamax          0\n",
      "laeq           0\n",
      "month          0\n",
      "day            0\n",
      "hour           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "''''\n",
    "# Combining the datasets \n",
    "combined_nov = pd.concat(dfs, ignore_index=True)\n",
    "del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "combined_nov['result_timestamp'] = pd.to_datetime(combined_nov['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "combined_nov['month'] = combined_nov['result_timestamp'].dt.month\n",
    "combined_nov['day'] = combined_nov['result_timestamp'].dt.day\n",
    "combined_nov['hour'] = combined_nov['result_timestamp'].dt.hour\n",
    "#combined_nov['minute'] = combined_nov['result_timestamp'].dt.minute\n",
    "\n",
    "# Drop the columns we won't use\n",
    "columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "columns_to_drop = set(combined_nov.columns) - set(columns_to_keep)\n",
    "combined_nov.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# check for missing values in each column\n",
    "print(combined_nov.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "nov_per_hour = combined_nov.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "nov_per_hour = nov_per_hour.reset_index()\n",
    "print(nov_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "combined_nov.drop('hour', axis=1, inplace=True)\n",
    "nov_per_day = combined_nov.groupby(['month', 'day', 'description']).mean()\n",
    "nov_per_day = nov_per_day.reset_index()\n",
    "print(nov_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "combined_nov.drop('day', axis=1, inplace=True)\n",
    "nov_per_month = combined_nov.groupby(['month', 'description']).mean()\n",
    "nov_per_month = nov_per_month.reset_index()\n",
    "print(nov_per_month.head())\n",
    "\n",
    "del combined_nov\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description    0\n",
      "lamax          0\n",
      "laeq           0\n",
      "month          0\n",
      "day            0\n",
      "hour           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Combining the datasets \n",
    "combined_dec = pd.concat(dfs, ignore_index=True)\n",
    "del dfs # deleting the separate dataframes to minimize memory usage\n",
    "\n",
    "# extract the month, day, hour, minute of \"result_timestamp\"\n",
    "combined_dec['result_timestamp'] = pd.to_datetime(combined_dec['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "combined_dec['month'] = combined_dec['result_timestamp'].dt.month\n",
    "combined_dec['day'] = combined_dec['result_timestamp'].dt.day\n",
    "combined_dec['hour'] = combined_dec['result_timestamp'].dt.hour\n",
    "#combined_dec['minute'] = combined_dec['result_timestamp'].dt.minute\n",
    "\n",
    "# Drop the columns we won't use\n",
    "columns_to_keep = ['description', 'lamax', 'laeq', 'month', 'day', 'hour'] #also minute if we calculate it\n",
    "columns_to_drop = set(combined_dec.columns) - set(columns_to_keep)\n",
    "combined_dec.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# check for missing values in each column\n",
    "print(combined_dec.isnull().sum())\n",
    "\n",
    "\n",
    "# Create dataframe per hour\n",
    "dec_per_hour = combined_dec.groupby(['month', 'day', 'hour', 'description']).mean()\n",
    "dec_per_hour = dec_per_hour.reset_index()\n",
    "print(dec_per_hour.head())\n",
    "\n",
    "# Create dataframe per day\n",
    "combined_dec.drop('hour', axis=1, inplace=True)\n",
    "dec_per_day = combined_dec.groupby(['month', 'day', 'description']).mean()\n",
    "dec_per_day = dec_per_day.reset_index()\n",
    "print(dec_per_day.head())\n",
    "\n",
    "# Create dataframe per month\n",
    "combined_dec.drop('day', axis=1, inplace=True)\n",
    "dec_per_month = combined_dec.groupby(['month', 'description']).mean()\n",
    "dec_per_month = dec_per_month.reset_index()\n",
    "print(dec_per_month.head())\n",
    "\n",
    "del combined_dec\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have exported the preprocessed dataframes for the noise and weather data of 2022, we can just use these files instead of loading all 112 files from the S3 bucket each time, as this takes a lot of time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD PREPROCESSING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data from the S3 bucket (don't forget to pip install boto3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # meteo data\n",
    "# Q1_2022 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Meteo+data/LC_2022Q1.csv')\n",
    "# Q2_2022 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Meteo+data/LC_2022Q2.csv')\n",
    "# Q3_2022 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Meteo+data/LC_2022Q3.csv')\n",
    "# Q4_2022 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Meteo+data/LC_2022Q4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMARK: this is the 'old' noise data, don't run this\n",
    "\n",
    "# noise data\n",
    "# exp40_naamse35 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_40/csv_results_40_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# exp40_naamse57 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_40/csv_results_40_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# exp40_naamse62 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_40/csv_results_40_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# exp40_calvarie = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_40/csv_results_40_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# exp40_naamse81 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_40/csv_results_40_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# exp40_park = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_40/csv_results_40_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# exp40_kiosk = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_40/csv_results_40_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# exp40_vrijt = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_40/csv_results_40_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# exp40_his = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_40/csv_results_40_303910_mp-04-his-hears.csv', header=0, sep=';')\n",
    "\n",
    "# exp41_naamse35 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_41/csv_results_41_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# exp41_naamse57 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_41/csv_results_41_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# exp41_naamse62 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_41/csv_results_41_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# exp41_calvarie = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_41/csv_results_41_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# exp41_naamse81 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_41/csv_results_41_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# exp41_park = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_41/csv_results_41_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# exp41_kiosk = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_41/csv_results_41_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# exp41_vrijt = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_41/csv_results_41_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# exp41_his = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_41/csv_results_41_303910_mp-04-his-hears.csv', header=0, sep=';')\n",
    "\n",
    "# exp42_naamse35 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_42/csv_results_42_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# exp42_naamse57 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_42/csv_results_42_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# exp42_naamse62 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_42/csv_results_42_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# exp42_calvarie = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_42/csv_results_42_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# exp42_naamse81 = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_42/csv_results_42_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# exp42_park = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_42/csv_results_42_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# exp42_kiosk = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_42/csv_results_42_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# exp42_vrijt = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_42/csv_results_42_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# exp42_his = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/export_42/csv_results_42_303910_mp-04-his-hears.csv', header=0, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_columns = [\"#object_id\", \"description\", \"result_timestamp\", \"lamax\", \"laeq\"]\n",
    "# naamse35_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';', usecols=noise_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - January\n",
    "# naamse35_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_jan = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jan/Jan/csv_results_42_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - February\n",
    "# naamse35_feb = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_feb = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_feb = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_feb = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_feb = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_feb = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_feb = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_feb = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_feb = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Feb/Feb/csv_results_42_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - March\n",
    "# naamse35_mar = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_mar = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_mar = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_mar = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_mar = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_mar = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_mar = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_mar = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_mar = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/March/March/csv_results_44_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - April\n",
    "# naamse35_apr = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_apr = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_apr = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_apr = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_apr = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_apr = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_apr = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_apr = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_apr = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/April/April/csv_results_45_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - May\n",
    "# naamse35_may = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_may = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_may = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_may = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_may = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_may = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_may = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_may = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_may = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/May/May/csv_results_46_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - June\n",
    "# naamse35_jun = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_jun = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_jun = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_jun = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_jun = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_jun = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_jun = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_jun = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_jun = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/June/June/csv_results_47_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - July\n",
    "# naamse35_jul = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_jul = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_jul = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_jul = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_jul = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_jul = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_jul = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_jul = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_jul = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Jul/Jul/csv_results_48_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - August\n",
    "# naamse35_aug = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_aug = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_aug = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_aug = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_aug = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_aug = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_aug = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_aug = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_aug = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Aug/Aug/csv_results_49_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - September\n",
    "# naamse35_sep = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_sep = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_sep = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_sep = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_sep = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_sep = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_sep = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_sep = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_sep = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Sep/Sep/csv_results_50_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - October\n",
    "# naamse35_oct = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_oct = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_oct = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_oct = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_oct = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_oct = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_oct = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_oct = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_oct = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Oct/Oct/csv_results_51_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - November\n",
    "# naamse35_nov = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_nov = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_nov = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_nov = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_nov = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_nov = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_nov = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_nov = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_nov = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Nov/Nov/csv_results_52_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # updated noise data - December\n",
    "# naamse35_dec = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255439_mp-01-naamsestraat-35-maxim.csv', header=0, sep=';')\n",
    "# naamse57_dec = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255440_mp-02-naamsestraat-57-xior.csv', header=0, sep=';')\n",
    "# naamse62_dec = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255441_mp-03-naamsestraat-62-taste.csv', header=0, sep=';')\n",
    "# calvarie_dec = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255442_mp-05-calvariekapel-ku-leuven.csv', header=0, sep=';')\n",
    "# park_dec = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255443_mp-06-parkstraat-2-la-filosovia.csv', header=0, sep=';')\n",
    "# naamse81_dec = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255444_mp-07-naamsestraat-81.csv', header=0, sep=';')\n",
    "# kiosk_dec = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_255445_mp-08-kiosk-stadspark.csv', header=0, sep=';')\n",
    "# vrijt_dec = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_280324_mp08bis---vrijthof.csv', header=0, sep=';')\n",
    "# his_dec = pd.read_csv('https://mda-georgia-bucket.s3.eu-central-1.amazonaws.com/Noise+data/Dec/Dec/csv_results_53_303910_mp-04-his-hears.csv', header=0, sep=';')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining and aggregating the meteo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine meteo dataset \n",
    "# meteocombined = pd.concat([Q1_2022, Q2_2022, Q3_2022, Q4_2022], axis=0)\n",
    "# meteocombined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values in each column\n",
    "# print(meteocombined.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate meteo data by day\n",
    "# avg_meteo_combined = meteocombined.groupby(['Year','Month', 'Day']).mean()\n",
    "# avg_meteo_combined = avg_meteo_combined.reset_index()\n",
    "# avg_meteo_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month_max_value = avg_meteo_combined['Month'].max()\n",
    "# print(f\"This combined meteo dataset contains the weather data for all {month_max_value} months.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining and aggregating the noise data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for January together\n",
    "# noise_jan_combined = pd.concat([naamse35_jan, naamse57_jan, naamse62_jan, calvarie_jan, park_jan, naamse81_jan, kiosk_jan, vrijt_jan, his_jan], axis=0)\n",
    "# noise_jan_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_jan_combined['result_timestamp'] = pd.to_datetime(noise_jan_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_jan_combined['result_date'] = noise_jan_combined['result_timestamp'].dt.date\n",
    "# noise_jan_combined['result_month'] = noise_jan_combined['result_timestamp'].dt.month\n",
    "# noise_jan_combined['result_day'] = noise_jan_combined['result_timestamp'].dt.day\n",
    "# noise_jan_combined['result_hour'] = noise_jan_combined['result_timestamp'].dt.hour\n",
    "# noise_jan_combined['result_minute'] = noise_jan_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_jan_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_jan_combined = noise_jan_combined.groupby(['result_date','description']).mean()\n",
    "# avg_jan_combined = avg_jan_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_jan_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_jan_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values in each column\n",
    "# print(avg_jan_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for February together\n",
    "# noise_feb_combined = pd.concat([naamse35_feb, naamse57_feb, naamse62_feb, calvarie_feb, park_feb, naamse81_feb, kiosk_feb, vrijt_feb, his_feb], axis=0)\n",
    "# noise_feb_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_feb_combined['result_timestamp'] = pd.to_datetime(noise_feb_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_feb_combined['result_date'] = noise_feb_combined['result_timestamp'].dt.date\n",
    "# noise_feb_combined['result_month'] = noise_feb_combined['result_timestamp'].dt.month\n",
    "# noise_feb_combined['result_day'] = noise_feb_combined['result_timestamp'].dt.day\n",
    "# noise_feb_combined['result_hour'] = noise_feb_combined['result_timestamp'].dt.hour\n",
    "# noise_feb_combined['result_minute'] = noise_feb_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_feb_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_feb_combined = noise_feb_combined.groupby(['result_date','description']).mean()\n",
    "# avg_feb_combined = avg_feb_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_feb_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_feb_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_feb_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for March together\n",
    "# noise_mar_combined = pd.concat([naamse35_mar, naamse57_mar, naamse62_mar, calvarie_mar, park_mar, naamse81_mar, kiosk_mar, vrijt_mar, his_mar], axis=0)\n",
    "# noise_mar_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_mar_combined['result_timestamp'] = pd.to_datetime(noise_mar_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_mar_combined['result_date'] = noise_mar_combined['result_timestamp'].dt.date\n",
    "# noise_mar_combined['result_month'] = noise_mar_combined['result_timestamp'].dt.month\n",
    "# noise_mar_combined['result_day'] = noise_mar_combined['result_timestamp'].dt.day\n",
    "# noise_mar_combined['result_hour'] = noise_mar_combined['result_timestamp'].dt.hour\n",
    "# noise_mar_combined['result_minute'] = noise_mar_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_mar_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_mar_combined = noise_mar_combined.groupby(['result_date','description']).mean()\n",
    "# avg_mar_combined = avg_mar_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_mar_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_mar_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_mar_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- April "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for April together\n",
    "# noise_apr_combined = pd.concat([naamse35_apr, naamse57_apr, naamse62_apr, calvarie_apr, park_apr, naamse81_apr, kiosk_apr, vrijt_apr, his_apr], axis=0)\n",
    "# noise_apr_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_apr_combined['result_timestamp'] = pd.to_datetime(noise_apr_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_apr_combined['result_date'] = noise_apr_combined['result_timestamp'].dt.date\n",
    "# noise_apr_combined['result_month'] = noise_apr_combined['result_timestamp'].dt.month\n",
    "# noise_apr_combined['result_day'] = noise_apr_combined['result_timestamp'].dt.day\n",
    "# noise_apr_combined['result_hour'] = noise_apr_combined['result_timestamp'].dt.hour\n",
    "# noise_apr_combined['result_minute'] = noise_apr_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_apr_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_apr_combined = noise_apr_combined.groupby(['result_date','description']).mean()\n",
    "# avg_apr_combined = avg_apr_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_apr_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_apr_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_apr_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- May "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for May together\n",
    "# noise_may_combined = pd.concat([naamse35_may, naamse57_may, naamse62_may, calvarie_may, park_may, naamse81_may, kiosk_may, vrijt_may, his_may], axis=0)\n",
    "# noise_may_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_may_combined['result_timestamp'] = pd.to_datetime(noise_may_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_may_combined['result_date'] = noise_may_combined['result_timestamp'].dt.date\n",
    "# noise_may_combined['result_month'] = noise_may_combined['result_timestamp'].dt.month\n",
    "# noise_may_combined['result_day'] = noise_may_combined['result_timestamp'].dt.day\n",
    "# noise_may_combined['result_hour'] = noise_may_combined['result_timestamp'].dt.hour\n",
    "# noise_may_combined['result_minute'] = noise_may_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_may_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_may_combined = noise_may_combined.groupby(['result_date','description']).mean()\n",
    "# avg_may_combined = avg_may_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_may_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_may_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_may_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for June together\n",
    "# noise_jun_combined = pd.concat([naamse35_jun, naamse57_jun, naamse62_jun, calvarie_jun, park_jun, naamse81_jun, kiosk_jun, vrijt_jun, his_jun], axis=0)\n",
    "# noise_jun_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_jun_combined['result_timestamp'] = pd.to_datetime(noise_jun_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_jun_combined['result_date'] = noise_jun_combined['result_timestamp'].dt.date\n",
    "# noise_jun_combined['result_month'] = noise_jun_combined['result_timestamp'].dt.month\n",
    "# noise_jun_combined['result_day'] = noise_jun_combined['result_timestamp'].dt.day\n",
    "# noise_jun_combined['result_hour'] = noise_jun_combined['result_timestamp'].dt.hour\n",
    "# noise_jun_combined['result_minute'] = noise_jun_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_jun_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the data by day\n",
    "# avg_jun_combined = noise_jun_combined.groupby(['result_date','description']).mean()\n",
    "# avg_jun_combined = avg_jun_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_jun_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_jun_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_jun_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for July together\n",
    "# noise_jul_combined = pd.concat([naamse35_jul, naamse57_jul, naamse62_jul, calvarie_jul, park_jul, naamse81_jul, kiosk_jul, vrijt_jul, his_jul], axis=0)\n",
    "# noise_jul_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_jul_combined['result_timestamp'] = pd.to_datetime(noise_jul_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_jul_combined['result_date'] = noise_jul_combined['result_timestamp'].dt.date\n",
    "# noise_jul_combined['result_month'] = noise_jul_combined['result_timestamp'].dt.month\n",
    "# noise_jul_combined['result_day'] = noise_jul_combined['result_timestamp'].dt.day\n",
    "# noise_jul_combined['result_hour'] = noise_jul_combined['result_timestamp'].dt.hour\n",
    "# noise_jul_combined['result_minute'] = noise_jul_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_jul_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_jul_combined = noise_jul_combined.groupby(['result_date','description']).mean()\n",
    "# avg_jul_combined = avg_jul_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_jul_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_jul_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_jul_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for August together\n",
    "# noise_aug_combined = pd.concat([naamse35_aug, naamse57_aug, naamse62_aug, calvarie_aug, park_aug, naamse81_aug, kiosk_aug, vrijt_aug, his_aug], axis=0)\n",
    "# noise_aug_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_aug_combined['result_timestamp'] = pd.to_datetime(noise_aug_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_aug_combined['result_date'] = noise_aug_combined['result_timestamp'].dt.date\n",
    "# noise_aug_combined['result_month'] = noise_aug_combined['result_timestamp'].dt.month\n",
    "# noise_aug_combined['result_day'] = noise_aug_combined['result_timestamp'].dt.day\n",
    "# noise_aug_combined['result_hour'] = noise_aug_combined['result_timestamp'].dt.hour\n",
    "# noise_aug_combined['result_minute'] = noise_aug_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_aug_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_aug_combined = noise_aug_combined.groupby(['result_date','description']).mean()\n",
    "# avg_aug_combined = avg_aug_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_aug_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_aug_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_aug_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- September"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for September together\n",
    "# noise_sep_combined = pd.concat([naamse35_sep, naamse57_sep, naamse62_sep, calvarie_sep, park_sep, naamse81_sep, kiosk_sep, vrijt_sep, his_sep], axis=0)\n",
    "# noise_sep_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_sep_combined['result_timestamp'] = pd.to_datetime(noise_sep_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_sep_combined['result_date'] = noise_sep_combined['result_timestamp'].dt.date\n",
    "# noise_sep_combined['result_month'] = noise_sep_combined['result_timestamp'].dt.month\n",
    "# noise_sep_combined['result_day'] = noise_sep_combined['result_timestamp'].dt.day\n",
    "# noise_sep_combined['result_hour'] = noise_sep_combined['result_timestamp'].dt.hour\n",
    "# noise_sep_combined['result_minute'] = noise_sep_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_sep_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_sep_combined = noise_sep_combined.groupby(['result_date','description']).mean()\n",
    "# avg_sep_combined = avg_sep_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_sep_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_sep_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_sep_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for Octber together\n",
    "# noise_oct_combined = pd.concat([naamse35_oct, naamse57_oct, naamse62_oct, calvarie_oct, park_oct, naamse81_oct, kiosk_oct, vrijt_oct, his_oct], axis=0)\n",
    "# noise_oct_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_oct_combined['result_timestamp'] = pd.to_datetime(noise_oct_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_oct_combined['result_date'] = noise_oct_combined['result_timestamp'].dt.date\n",
    "# noise_oct_combined['result_month'] = noise_oct_combined['result_timestamp'].dt.month\n",
    "# noise_oct_combined['result_day'] = noise_oct_combined['result_timestamp'].dt.day\n",
    "# noise_oct_combined['result_hour'] = noise_oct_combined['result_timestamp'].dt.hour\n",
    "# noise_oct_combined['result_minute'] = noise_oct_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_oct_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_oct_combined = noise_oct_combined.groupby(['result_date','description']).mean()\n",
    "# avg_oct_combined = avg_oct_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_oct_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_oct_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_oct_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for November together\n",
    "# noise_nov_combined = pd.concat([naamse35_nov, naamse57_nov, naamse62_nov, calvarie_nov, park_nov, naamse81_nov, kiosk_nov, vrijt_nov, his_nov], axis=0)\n",
    "# noise_nov_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_nov_combined['result_timestamp'] = pd.to_datetime(noise_nov_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_nov_combined['result_date'] = noise_nov_combined['result_timestamp'].dt.date\n",
    "# noise_nov_combined['result_month'] = noise_nov_combined['result_timestamp'].dt.month\n",
    "# noise_nov_combined['result_day'] = noise_nov_combined['result_timestamp'].dt.day\n",
    "# noise_nov_combined['result_hour'] = noise_nov_combined['result_timestamp'].dt.hour\n",
    "# noise_nov_combined['result_minute'] = noise_nov_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_nov_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_nov_combined = noise_nov_combined.groupby(['result_date','description']).mean()\n",
    "# avg_nov_combined = avg_nov_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_nov_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_nov_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_nov_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine noise data for December together\n",
    "# noise_dec_combined = pd.concat([naamse35_dec, naamse57_dec, naamse62_dec, calvarie_dec, park_dec, naamse81_dec, kiosk_dec, vrijt_dec, his_dec], axis=0)\n",
    "# noise_dec_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the date, month, hour, minute of \"result_timestamp\"\n",
    "# noise_dec_combined['result_timestamp'] = pd.to_datetime(noise_dec_combined['result_timestamp'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "# noise_dec_combined['result_date'] = noise_dec_combined['result_timestamp'].dt.date\n",
    "# noise_dec_combined['result_month'] = noise_dec_combined['result_timestamp'].dt.month\n",
    "# noise_dec_combined['result_day'] = noise_dec_combined['result_timestamp'].dt.day\n",
    "# noise_dec_combined['result_hour'] = noise_dec_combined['result_timestamp'].dt.hour\n",
    "# noise_dec_combined['result_minute'] = noise_dec_combined['result_timestamp'].dt.minute\n",
    "\n",
    "# noise_dec_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate the data by day\n",
    "# avg_dec_combined = noise_dec_combined.groupby(['result_date','description']).mean()\n",
    "# avg_dec_combined = avg_dec_combined.reset_index()\n",
    "# columns_to_drop = ['result_hour', 'result_minute']\n",
    "# avg_dec_combined.drop(columns_to_drop, axis=1, inplace=True)\n",
    "# avg_dec_combined.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for missing values in each column\n",
    "# print(avg_dec_combined.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining monthly noise level datasets into a yearly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the monthly datasets\n",
    "datasets = [avg_jan_combined, avg_feb_combined, avg_mar_combined, avg_apr_combined, avg_may_combined, avg_jun_combined, avg_jul_combined, avg_aug_combined, avg_sep_combined, avg_oct_combined, avg_nov_combined, avg_dec_combined]\n",
    "\n",
    "# Concatenate the datasets vertically\n",
    "avg_year_combined = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Sort the combined dataset by 'result_date' in ascending order\n",
    "avg_year_combined.sort_values(by='result_date', inplace=True)\n",
    "\n",
    "# Reset the index of the combined dataset\n",
    "avg_year_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the combined and sorted yearly dataset\n",
    "avg_year_combined.head(2000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
